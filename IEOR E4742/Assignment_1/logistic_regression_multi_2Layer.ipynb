{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# generate original training and test data\n",
    "img_size = 28\n",
    "n_classes = 10\n",
    "\n",
    "#MNIST data image of shape 28*28=784\n",
    "input_size = 784\n",
    "\n",
    "# 0-9 digits recognition (labels)\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading MNIST\n",
      "\n",
      "Spliting data\n",
      "54000 6000 10000\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------\n",
    "#option 1: load MNIST dataset \n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "#option 2: load MNIST dataset \n",
    "print('\\nLoading MNIST')\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = np.reshape(x_train, [-1, img_size*img_size])\n",
    "x_train = x_train.astype(np.float32)/255\n",
    "\n",
    "x_test = np.reshape(x_test, [-1, img_size*img_size])\n",
    "x_test = x_test.astype(np.float32)/255\n",
    "\n",
    "to_categorical = tf.keras.utils.to_categorical \n",
    "y_train = to_categorical(y_train)\n",
    "y_test  = to_categorical(y_test)\n",
    "\n",
    "print('\\nSpliting data')\n",
    "\n",
    "ind = np.random.permutation(x_train.shape[0])\n",
    "x_train, y_train = x_train[ind], y_train[ind]\n",
    "\n",
    "# 10% for validation \n",
    "validatationPct = 0.1\n",
    "n = int(x_train.shape[0] * (1-validatationPct))\n",
    "x_valid = x_train[n:]\n",
    "x_train = x_train[:n]\n",
    "#\n",
    "y_valid = y_train[n:]\n",
    "y_train = y_train[:n]\n",
    "\n",
    "train_num_examples = x_train.shape[0]\n",
    "valid_num_examples = x_valid.shape[0]\n",
    "test_num_examples  = x_test.shape[0]\n",
    "\n",
    "print(train_num_examples, valid_num_examples, test_num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.107135Z",
     "start_time": "2018-09-17T22:06:31.090083Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "#--------------------------------\n",
    "# learning rate\n",
    "learning_rate = 0.05\n",
    "\n",
    "#training_epochs = 1000\n",
    "#batch_size = 30\n",
    "\n",
    "training_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "display_step = 10\n",
    "\n",
    "#Network Architecture\n",
    "# -----------------------------------------\n",
    "#\n",
    "# Two hidden layers\n",
    "#\n",
    "#------------------------------------------\n",
    "# number of neurons in layer 1\n",
    "n_hidden_1 = 200\n",
    "# number of neurons in layer 2\n",
    "n_hidden_2 = 300\n",
    "\n",
    "#MNIST data image of shape 28*28=784\n",
    "input_size = 784\n",
    "\n",
    "# 0-9 digits recognition (labels)\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Layer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.158231Z",
     "start_time": "2018-09-17T22:06:31.126508Z"
    }
   },
   "outputs": [],
   "source": [
    "def layer(x, weight_shape, bias_shape):\n",
    "    \"\"\"\n",
    "    Defines the network layers.\n",
    "    \n",
    "    Input:\n",
    "        - x: input vector of the layer\n",
    "        - weight_shape: shape of the weight matrix\n",
    "        - bias_shape: shape of the bias vector\n",
    "    Output:\n",
    "        - output: the output of the layer after matrix multiplication and non-linear transformation\n",
    "    \"\"\"\n",
    "    # Comes from the study by He et al. for ReLU layers\n",
    "    w_std = (2.0 / weight_shape[0])**0.5\n",
    "\n",
    "    # Initialize weights and biases using TensorFlow's built-in initializers\n",
    "    initializer_w = tf.random_normal_initializer(stddev=w_std)\n",
    "    initializer_b = tf.zeros_initializer()\n",
    "\n",
    "    # Create weights and bias variables\n",
    "    W = tf.Variable(initializer_w(shape=weight_shape), trainable=True, name=\"W\")\n",
    "    b = tf.Variable(initializer_b(shape=bias_shape), trainable=True, name=\"b\")\n",
    "\n",
    "    print('Weight Matrix:', W)\n",
    "    print('Bias Vector:', b)\n",
    "\n",
    "    # Apply matrix multiplication and ReLU activation\n",
    "    return tf.nn.relu(tf.matmul(x, W) + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.193619Z",
     "start_time": "2018-09-17T22:06:31.176269Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(x, input_size, n_hidden_1, n_hidden_2, output_size):\n",
    "    \"\"\"\n",
    "    Defines a neural network with two hidden layers and an output layer.\n",
    "    \n",
    "    Input:\n",
    "        - x: a batch of input features (input shape = (batch_size, input_size))\n",
    "    Output:\n",
    "        - logits: the output of the network before applying activation (logits)\n",
    "                  (output shape = (batch_size, output_size))\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hidden layer 1\n",
    "    hidden_1 = tf.keras.layers.Dense(n_hidden_1, activation='relu', name=\"hidden_layer_1\")(x)\n",
    "    \n",
    "    # Hidden layer 2\n",
    "    hidden_2 = tf.keras.layers.Dense(n_hidden_2, activation='relu', name=\"hidden_layer_2\")(hidden_1)\n",
    "    \n",
    "    # Output layer (logits)\n",
    "    output = tf.keras.layers.Dense(output_size, name=\"output\")(hidden_2)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define First Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.251904Z",
     "start_time": "2018-09-17T22:06:31.235310Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_1(output, y):\n",
    "    \"\"\"\n",
    "    Computes the average error per data sample \n",
    "    by computing the cross-entropy loss over a minibatch.\n",
    "    \n",
    "    Input:\n",
    "        - output: the output of the inference function (logits)\n",
    "        - y: true labels of the sample batch (one-hot encoded)\n",
    "    Output:\n",
    "        - loss: scalar tensor representing the loss for the batch\n",
    "    \"\"\"\n",
    "    # Compute the log of the output (logits are expected to be probabilities or softmax outputs)\n",
    "    log_output = tf.math.log(output)\n",
    "    \n",
    "    # Compute the element-wise product of true labels and log output\n",
    "    dot_product = y * log_output\n",
    "    \n",
    "    # Sum the negative log-likelihoods across the class dimension (axis 1)\n",
    "    xentropy = -tf.reduce_sum(dot_product, axis=1)\n",
    "    \n",
    "    # Compute the mean loss across the batch\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T20:17:03.371287Z",
     "start_time": "2018-09-17T20:17:03.367055Z"
    }
   },
   "source": [
    "## Define Second Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.284838Z",
     "start_time": "2018-09-17T22:06:31.276738Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_2(output, y):\n",
    "    \"\"\"\n",
    "    Computes softmax cross entropy between logits and labels and returns the loss.\n",
    "    \n",
    "    Input:\n",
    "        - output: the output (logits) of the inference function (shape: batch_size * num_of_classes)\n",
    "        - y: true labels for the sample batch (shape: batch_size * num_of_classes)\n",
    "    Output:\n",
    "        - loss: the scalar loss value for the batch\n",
    "    \"\"\"\n",
    "    # Computes softmax cross entropy between logits (output) and true labels (y)\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y)\n",
    "    \n",
    "    # Return the mean cross-entropy loss across the batch\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the optimizer and training target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.313660Z",
     "start_time": "2018-09-17T22:06:31.305501Z"
    }
   },
   "outputs": [],
   "source": [
    "def training(cost, global_step):\n",
    "    \"\"\"\n",
    "    Defines the necessary elements to train the network.\n",
    "    \n",
    "    Input:\n",
    "        - cost: the loss of the corresponding batch\n",
    "        - global_step: the number of batches seen so far\n",
    "    \"\"\"\n",
    "    # Define a scalar summary for the cost (loss) value\n",
    "    with tf.summary.create_file_writer('./logs/training').as_default():\n",
    "        tf.summary.scalar(\"cost\", cost, step=global_step)\n",
    "    \n",
    "    # Use the new optimizer from TensorFlow 2.x\n",
    "    optimizer = tf.optimizers.SGD(learning_rate)\n",
    "    \n",
    "    # Define the training step\n",
    "    train_op = optimizer.minimize(cost, var_list=tf.trainable_variables())\n",
    "    \n",
    "    # Increment the global step manually (if required)\n",
    "    global_step.assign_add(1)\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.556011Z",
     "start_time": "2018-09-17T22:06:31.541378Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    \"\"\"\n",
    "    Evaluates the accuracy on the validation set.\n",
    "    Input:\n",
    "        - output: prediction vector of the network for the validation set\n",
    "        - y: true value for the validation set\n",
    "    Output:\n",
    "        - accuracy: accuracy on the validation set (scalar between 0 and 1)\n",
    "    \"\"\"\n",
    "    # Check if the predicted class equals the true class\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Compute accuracy as the mean of correct predictions\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # Log validation accuracy using TensorFlow summary (if needed)\n",
    "    with tf.summary.create_file_writer('./logs/validation').as_default():\n",
    "        tf.summary.scalar(\"validation_error\", 1.0 - accuracy, step=0)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:27:56.024948Z",
     "start_time": "2018-09-17T22:27:29.279712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, cost=0.2860187, Validation Error=0.0448334\n",
      "Epoch: 001, cost=0.1069164, Validation Error=0.0340000\n",
      "Epoch: 002, cost=0.0695702, Validation Error=0.0333334\n",
      "Epoch: 003, cost=0.0484154, Validation Error=0.0308333\n",
      "Epoch: 004, cost=0.0346433, Validation Error=0.0265000\n",
      "Epoch: 005, cost=0.0270449, Validation Error=0.0271667\n",
      "Epoch: 006, cost=0.0248251, Validation Error=0.0263333\n",
      "Epoch: 007, cost=0.0223597, Validation Error=0.0283333\n",
      "Epoch: 008, cost=0.0197223, Validation Error=0.0246667\n",
      "Epoch: 009, cost=0.0152146, Validation Error=0.0235000\n",
      "Epoch: 010, cost=0.0099945, Validation Error=0.0246667\n",
      "Epoch: 011, cost=0.0103586, Validation Error=0.0236667\n",
      "Epoch: 012, cost=0.0111240, Validation Error=0.0263333\n",
      "Epoch: 013, cost=0.0107814, Validation Error=0.0243334\n",
      "Epoch: 014, cost=0.0102970, Validation Error=0.0253333\n",
      "Epoch: 015, cost=0.0098749, Validation Error=0.0253333\n",
      "Epoch: 016, cost=0.0078512, Validation Error=0.0251667\n",
      "Epoch: 017, cost=0.0058092, Validation Error=0.0250000\n",
      "Epoch: 018, cost=0.0108361, Validation Error=0.0301667\n",
      "Epoch: 019, cost=0.0090588, Validation Error=0.0223333\n",
      "Test Accuracy: tf.Tensor(0.9803, shape=(), dtype=float32)\n",
      "Execution time (seconds) was 355.261\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not os.path.isdir('./logs/'):\n",
    "        os.makedirs('./logs/')\n",
    "    log_files_path = './logs/'\n",
    "\n",
    "    # Define inputs directly (no need for placeholders)\n",
    "    input_size = 784\n",
    "    output_size = 10\n",
    "    batch_size = 128\n",
    "    training_epochs = 20\n",
    "    display_step = 1\n",
    "    n_hidden_1 = 200\n",
    "    n_hidden_2 = 300\n",
    "    \n",
    "    # Define your model using a custom class\n",
    "    class MyModel(tf.keras.Model):\n",
    "        def __init__(self, input_size, n_hidden_1, n_hidden_2, output_size):\n",
    "            super(MyModel, self).__init__()\n",
    "            # Define layers\n",
    "            self.hidden_1 = tf.keras.layers.Dense(n_hidden_1, activation='relu')\n",
    "            self.hidden_2 = tf.keras.layers.Dense(n_hidden_2, activation='relu')\n",
    "            self.output_layer = tf.keras.layers.Dense(output_size)\n",
    "        \n",
    "        def call(self, inputs):\n",
    "            # Forward pass\n",
    "            x = self.hidden_1(inputs)\n",
    "            x = self.hidden_2(x)\n",
    "            return self.output_layer(x)\n",
    "\n",
    "    # Instantiate the model with the architecture parameters\n",
    "    model = MyModel(input_size, n_hidden_1, n_hidden_2, output_size)\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = tf.optimizers.Adam()\n",
    "\n",
    "    # Define the checkpoint manager\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "    checkpoint_manager = tf.train.CheckpointManager(checkpoint, './logs/multi_layer', max_to_keep=5)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        avg_cost = 0.\n",
    "        total_batch = int((train_num_examples + batch_size - 1) / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            start = i * batch_size\n",
    "            end = min(train_num_examples, start + batch_size)\n",
    "            minibatch_x = x_train[start:end]\n",
    "            minibatch_y = y_train[start:end]\n",
    "            \n",
    "            # Define training step using GradientTape\n",
    "            with tf.GradientTape() as tape:\n",
    "                output = model(minibatch_x)\n",
    "                cost = loss_2(output, minibatch_y)\n",
    "            \n",
    "            # Compute gradients and apply them\n",
    "            gradients = tape.gradient(cost, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "            avg_cost += cost.numpy() / total_batch\n",
    "\n",
    "        if epoch % display_step == 0:\n",
    "            # Evaluate on validation data\n",
    "            accuracy = evaluate(model(x_valid), y_valid)\n",
    "            print(f\"Epoch: {epoch:03d}, cost={avg_cost:.7f}, Validation Error={1-accuracy:.7f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint_manager.save()\n",
    "\n",
    "    # Final test accuracy\n",
    "    accuracy = evaluate(model(x_test), y_test)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Execution time (seconds) was {elapsed_time:.3f}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
