{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "284e071c-5fd1-452f-a55d-2d9640dc13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import scipy.stats as stats\n",
    "# import tensorflow_probability as tfp\n",
    "# import gpytorch\n",
    "# from gpytorch.distributions import MultivariateNormal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import gamma, norm, beta\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder,  MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e5e4a88-1603-47b4-ae66-ef8f7b5cacc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ucimlrepo\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5cd352-757b-4370-8ccb-08daf15869dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5204855-0b0d-43d2-a35a-199b1bc4eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "statlog_german_credit_data = fetch_ucirepo(id=144) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "data = statlog_german_credit_data.data.features \n",
    "y = statlog_german_credit_data.data.targets.values - 1 ## 1 subtracted for (0 = Good,  1 = Bad) labelling\n",
    "  \n",
    "# metadata \n",
    "#print(statlog_german_credit_data.metadata) \n",
    "  \n",
    "# variable information \n",
    "#print(statlog_german_credit_data.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbea97dd-5324-4a25-85dc-728348ab5669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data[statlog_german_credit_data.variables.name[:-1]]\n",
    "df_full=df.copy()\n",
    "df_full.columns=statlog_german_credit_data.variables.description[:-1].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cef8bfde-a668-41ab-b146-048a693a8c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute2</th>\n",
       "      <th>Attribute5</th>\n",
       "      <th>Attribute8</th>\n",
       "      <th>Attribute11</th>\n",
       "      <th>Attribute13</th>\n",
       "      <th>Attribute16</th>\n",
       "      <th>Attribute18</th>\n",
       "      <th>x0_A11</th>\n",
       "      <th>x0_A12</th>\n",
       "      <th>x0_A13</th>\n",
       "      <th>...</th>\n",
       "      <th>x8_A143</th>\n",
       "      <th>x9_A151</th>\n",
       "      <th>x9_A152</th>\n",
       "      <th>x9_A153</th>\n",
       "      <th>x10_A171</th>\n",
       "      <th>x10_A172</th>\n",
       "      <th>x10_A173</th>\n",
       "      <th>x10_A174</th>\n",
       "      <th>Attribute19</th>\n",
       "      <th>Attribute20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.941176</td>\n",
       "      <td>-0.898867</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>-0.372620</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.892857</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-0.796853</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>-0.160119</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.411765</td>\n",
       "      <td>-0.491581</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-0.836470</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.571429</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-0.235294</td>\n",
       "      <td>-0.603059</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-0.939034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.321429</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>-0.824475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>-0.523935</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Attribute2  Attribute5  Attribute8  Attribute11  Attribute13  \\\n",
       "0     -0.941176   -0.898867    1.000000     1.000000     0.714286   \n",
       "1      0.294118   -0.372620   -0.333333    -0.333333    -0.892857   \n",
       "2     -0.764706   -0.796853   -0.333333     0.333333     0.071429   \n",
       "3      0.117647   -0.160119   -0.333333     1.000000    -0.071429   \n",
       "4     -0.411765   -0.491581    0.333333     1.000000     0.214286   \n",
       "..          ...         ...         ...          ...          ...   \n",
       "995   -0.764706   -0.836470    0.333333     1.000000    -0.571429   \n",
       "996   -0.235294   -0.603059    1.000000     1.000000    -0.250000   \n",
       "997   -0.764706   -0.939034    1.000000     1.000000    -0.321429   \n",
       "998    0.205882   -0.824475    1.000000     1.000000    -0.857143   \n",
       "999    0.205882   -0.523935    0.333333     1.000000    -0.714286   \n",
       "\n",
       "     Attribute16  Attribute18  x0_A11  x0_A12  x0_A13  ...  x8_A143  x9_A151  \\\n",
       "0      -0.333333         -1.0     1.0    -1.0    -1.0  ...      1.0     -1.0   \n",
       "1      -1.000000         -1.0    -1.0     1.0    -1.0  ...      1.0     -1.0   \n",
       "2      -1.000000          1.0    -1.0    -1.0    -1.0  ...      1.0     -1.0   \n",
       "3      -1.000000          1.0     1.0    -1.0    -1.0  ...      1.0     -1.0   \n",
       "4      -0.333333          1.0     1.0    -1.0    -1.0  ...      1.0     -1.0   \n",
       "..           ...          ...     ...     ...     ...  ...      ...      ...   \n",
       "995    -1.000000         -1.0    -1.0    -1.0    -1.0  ...      1.0     -1.0   \n",
       "996    -1.000000         -1.0     1.0    -1.0    -1.0  ...      1.0     -1.0   \n",
       "997    -1.000000         -1.0    -1.0    -1.0    -1.0  ...      1.0     -1.0   \n",
       "998    -1.000000         -1.0     1.0    -1.0    -1.0  ...      1.0     -1.0   \n",
       "999    -1.000000         -1.0    -1.0     1.0    -1.0  ...      1.0     -1.0   \n",
       "\n",
       "     x9_A152  x9_A153  x10_A171  x10_A172  x10_A173  x10_A174  Attribute19  \\\n",
       "0        1.0     -1.0      -1.0      -1.0       1.0      -1.0          1.0   \n",
       "1        1.0     -1.0      -1.0      -1.0       1.0      -1.0         -1.0   \n",
       "2        1.0     -1.0      -1.0       1.0      -1.0      -1.0         -1.0   \n",
       "3       -1.0      1.0      -1.0      -1.0       1.0      -1.0         -1.0   \n",
       "4       -1.0      1.0      -1.0      -1.0       1.0      -1.0         -1.0   \n",
       "..       ...      ...       ...       ...       ...       ...          ...   \n",
       "995      1.0     -1.0      -1.0       1.0      -1.0      -1.0         -1.0   \n",
       "996      1.0     -1.0      -1.0      -1.0      -1.0       1.0          1.0   \n",
       "997      1.0     -1.0      -1.0      -1.0       1.0      -1.0         -1.0   \n",
       "998     -1.0      1.0      -1.0      -1.0       1.0      -1.0          1.0   \n",
       "999      1.0     -1.0      -1.0      -1.0       1.0      -1.0         -1.0   \n",
       "\n",
       "     Attribute20  \n",
       "0           -1.0  \n",
       "1           -1.0  \n",
       "2           -1.0  \n",
       "3           -1.0  \n",
       "4           -1.0  \n",
       "..           ...  \n",
       "995         -1.0  \n",
       "996         -1.0  \n",
       "997         -1.0  \n",
       "998         -1.0  \n",
       "999         -1.0  \n",
       "\n",
       "[1000 rows x 59 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define preprocessing steps\n",
    "numeric_features = ['Attribute2', 'Attribute5', 'Attribute8', 'Attribute11', 'Attribute13', 'Attribute16', 'Attribute18']\n",
    "binary_features = ['Attribute19', 'Attribute20']\n",
    "categorical_features = ['Attribute1', 'Attribute3', 'Attribute4', 'Attribute6', 'Attribute7', 'Attribute9', 'Attribute10', 'Attribute12', 'Attribute14', 'Attribute15', 'Attribute17']\n",
    "\n",
    "# Apply LabelEncoder to binary features\n",
    "label_encoders = {}\n",
    "for feature in binary_features:\n",
    "    le = LabelEncoder()\n",
    "    df[feature] = le.fit_transform(df[feature])\n",
    "    label_encoders[feature] = le  # Store the encoder for future use (e.g., inverse transform)\n",
    "\n",
    "# Pipeline for numeric features: Imputation and Min-Max Scaling between -1 and 1\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler(feature_range=(-1, 1)))\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features: Imputation, OneHotEncoding, and Min-Max Scaling between -1 and 1\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False)),  # Set sparse=False for easy concatenation\n",
    "    ('scaler', MinMaxScaler(feature_range=(-1, 1)))  # Scaling the one-hot encoded features\n",
    "])\n",
    "\n",
    "# For binary features, use Min-Max Scaling as well\n",
    "binary_transformer = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler(feature_range=(-1, 1)))\n",
    "])\n",
    "\n",
    "# Combine the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('bin', binary_transformer, binary_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the preprocessing steps to the DataFrame\n",
    "df2 = preprocessor.fit_transform(df)\n",
    "\n",
    "# If you want to convert it back to a DataFrame for ease of use\n",
    "# Create column names for the one-hot encoded features\n",
    "onehot_feature_names = list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out())\n",
    "\n",
    "# Combine all feature names\n",
    "all_feature_names = numeric_features + onehot_feature_names + binary_features\n",
    "\n",
    "# Create the processed DataFrame\n",
    "df2 = pd.DataFrame(df2, columns=all_feature_names)\n",
    "\n",
    "# Show the processed DataFrame\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab017bc7-5d1a-47ab-a9fa-3043144988ae",
   "metadata": {},
   "source": [
    "## 2. Define linear reward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3aeb8850-86f9-4071-9fac-21ab650a6927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Call the seed setting function\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b13ba5c-4cbf-4aa7-8709-38182501a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RewardGenerator and generate_data functions (from your code)\n",
    "class RewardGenerator:\n",
    "    def __init__(self, drift_rate=0.005, seasonal_period=400):\n",
    "        self.drift_rate = drift_rate\n",
    "        self.seasonal_period = seasonal_period\n",
    "        self.t = 0  # Initialize time step\n",
    "\n",
    "    # def generate_rewards1(self, context, action, optimal_action, n=3):\n",
    "    #     '''\n",
    "    #     Generates rewards with added complexity from context sampling and non-stationarity.\n",
    "\n",
    "    #     Parameters:\n",
    "    #     - context: np.array, the feature context\n",
    "    #     - action: int, the action taken\n",
    "    #     - optimal_action: int, the optimal action for this context\n",
    "    #     - n: int, number of context features to sample for reward generation\n",
    "\n",
    "    #     Returns:\n",
    "    #     - reward: float, the generated reward\n",
    "    #     '''\n",
    "    #     # Sample `n` context features\n",
    "    #     if len(context) >= n:\n",
    "    #         sampled_context = np.random.choice(context, size=n, replace=False)\n",
    "    #     else:\n",
    "    #         sampled_context = context  # Use all features if fewer than n\n",
    "        \n",
    "    #     # Compute a context factor based on the sampled features\n",
    "    #     context_factor = np.sum(sampled_context)\n",
    "        \n",
    "    #     # Non-stationarity (drift and seasonality)\n",
    "    #     time_drift = self.drift_rate * self.t\n",
    "    #     seasonality = np.sin(2 * np.pi * self.t / self.seasonal_period)\n",
    "\n",
    "    #     # Calculate reward based on action and non-stationary factors\n",
    "    #     if action == optimal_action:\n",
    "    #         mean = 1 + 0.2 * context_factor + time_drift + seasonality\n",
    "    #         std = 0.1  # Small variance for optimal action\n",
    "    #     else:\n",
    "    #         mean = 0 + 0.5 * context_factor + time_drift + seasonality\n",
    "    #         std = 0.4  # Higher variance for non-optimal action\n",
    "        \n",
    "    #     # Sample reward from Gaussian distribution\n",
    "    #     reward = np.random.normal(mean, std)\n",
    "\n",
    "    #     # Increment time step for next call\n",
    "    #     self.t += 1\n",
    "        \n",
    "    #     return reward\n",
    "    \n",
    "    def generate_rewards1(self, context, action, optimal_action, n = 3):\n",
    "        '''\n",
    "        Generates rewards using a simple linear function with noise.\n",
    "\n",
    "        Parameters:\n",
    "        - context: np.array, the feature context\n",
    "        - action: int, the action taken\n",
    "        - optimal_action: int, the optimal action for this context\n",
    "\n",
    "        Returns:\n",
    "        - reward: float, the generated reward\n",
    "        '''\n",
    "\n",
    "        # Sample `n` context features\n",
    "        if len(context) >= n:\n",
    "            sampled_context = np.random.choice(context, size=n, replace=False)\n",
    "        else:\n",
    "            sampled_context = context  # Use all features if fewer than n\n",
    "        \n",
    "        # Linear reward based on context sum\n",
    "        context_factor = np.sum(sampled_context)\n",
    "        \n",
    "        # Add a base reward depending on whether the action is optimal\n",
    "        base_reward = 1.0 if action == optimal_action else 0.5\n",
    "\n",
    "        # Linear reward calculation with noise\n",
    "        reward = base_reward + 0.1 * context_factor + np.random.normal(0, 0.1)\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def generate_rewards2(self, context, action, optimal_action, n=3):\n",
    "        '''\n",
    "        Generates rewards using a piecewise linear function with context-dependent intervals.\n",
    "        '''\n",
    "        # Sample `n` context features\n",
    "        if len(context) >= n:\n",
    "            sampled_context = np.random.choice(context, size=n, replace=False)\n",
    "        else:\n",
    "            sampled_context = context  # Use all features if fewer than n\n",
    "\n",
    "        # Compute a context factor based on the sampled features\n",
    "        context_factor = np.sum(sampled_context)\n",
    "        \n",
    "        # Piecewise modification based on context factor\n",
    "        if context_factor < 0.5:\n",
    "            context_factor = 1\n",
    "        elif 0.5 <= context_factor < 0.6:\n",
    "            context_factor = -10 * context_factor + 4\n",
    "        else:\n",
    "            context_factor = 0\n",
    "        \n",
    "        # Non-stationarity (drift and seasonality)\n",
    "        time_drift = self.drift_rate * self.t\n",
    "        seasonality = np.sin(2 * np.pi * self.t / self.seasonal_period)\n",
    "\n",
    "        # Calculate reward based on action and non-stationary factors\n",
    "        if action == optimal_action:\n",
    "            mean = 1 + 0.3 * context_factor + time_drift + seasonality\n",
    "            std = 0.1  # Small variance for optimal action\n",
    "        else:\n",
    "            mean = 0 + 0.6 * context_factor + time_drift + seasonality\n",
    "            std = 0.3  # Higher variance for non-optimal action\n",
    "        \n",
    "        # Sample reward from Gaussian distribution\n",
    "        reward = np.random.normal(mean, std)\n",
    "\n",
    "        # Increment time step for next call\n",
    "        self.t += 1\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def generate_rewards3(self, context, action, optimal_action, n=3):\n",
    "        '''\n",
    "        Generates rewards using a piecewise linear function with context-dependent intervals.\n",
    "        '''\n",
    "        # Sample `n` context features\n",
    "        if len(context) >= n:\n",
    "            sampled_context = np.random.choice(context, size=n, replace=False)\n",
    "        else:\n",
    "            sampled_context = context  # Use all features if fewer than n\n",
    "        \n",
    "        # Compute a context factor based on the sampled features\n",
    "        context_factor = np.sum(sampled_context)\n",
    "        \n",
    "        # Non-stationarity (drift and oscillation)\n",
    "        time_drift = self.drift_rate * self.t\n",
    "        # Oscillation based on time\n",
    "        oscillation = 2 * np.cos(5 * np.pi * self.t / self.seasonal_period)\n",
    "\n",
    "        # Calculate reward based on action and non-stationary factors\n",
    "        if action == optimal_action:\n",
    "            mean = 1 + 0.2 * context_factor + time_drift + oscillation\n",
    "            std = 0.15  # Small variance for optimal action\n",
    "        else:\n",
    "            mean = 0 + 0.4 * context_factor + time_drift + oscillation\n",
    "            std = 0.35  # Higher variance for non-optimal action\n",
    "        \n",
    "        # Sample reward from Gaussian distribution\n",
    "        reward = np.random.normal(mean, std)\n",
    "\n",
    "        # Increment time step for next call\n",
    "        self.t += 1\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def generate_rewards4(self, context, action, optimal_action, n=3):\n",
    "        '''\n",
    "        Generates rewards with a weighted linear combination of context features, drift, and seasonality.\n",
    "        '''\n",
    "        # Sample `n` context features\n",
    "        if len(context) >= n:\n",
    "            sampled_context = np.random.choice(context, size=n, replace=False)\n",
    "        else:\n",
    "            sampled_context = context  # Use all features if fewer than n\n",
    "        \n",
    "        # Assign weights to context features\n",
    "        context_weights = np.array([0.3, 0.5, 0.2])[:len(sampled_context)]\n",
    "        # Weighted sum of context features\n",
    "        context_factor = np.dot(sampled_context, context_weights)  \n",
    "\n",
    "        # Non-stationary components: drift and seasonality\n",
    "        time_drift = self.drift_rate * self.t  \n",
    "        seasonality = 1.5 * np.sin(2 * np.pi * self.t / (self.seasonal_period * 4))\n",
    "\n",
    "        # Calculate reward based on action and non-stationary factors\n",
    "        if action == optimal_action:\n",
    "            mean = 1.2 + 0.4 * context_factor + time_drift + seasonality\n",
    "            std = 0.1  # Small variance for optimal action\n",
    "        else:\n",
    "            mean = 0.5 + 0.7 * context_factor + time_drift + seasonality\n",
    "            std = 0.3  # Higher variance for non-optimal action\n",
    "\n",
    "        # Sample reward from Gaussian distribution\n",
    "        reward = np.random.normal(mean, std)\n",
    "\n",
    "        # Increment time step for next call\n",
    "        self.t += 1\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72c9df6a-9203-4048-938f-7c66d1088085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage in your data generation function\n",
    "def generate_data(context, total_samples, n_actions, optimal_actions, reward_type):\n",
    "    rewards = torch.zeros(total_samples, n_actions)\n",
    "    \n",
    "    reward_gen = RewardGenerator()\n",
    "\n",
    "    for i in range(total_samples):\n",
    "        for action in range(n_actions):\n",
    "            # Generate rewards based on the given logic\n",
    "            if reward_type == 'lin1':\n",
    "                rewards[i, action] = reward_gen.generate_rewards1(context[i], action,  optimal_actions[i])\n",
    "            elif reward_type == 'lin2':\n",
    "                rewards[i, action] = reward_gen.generate_rewards2(context[i], action,  optimal_actions[i])\n",
    "            elif reward_type == 'lin3':\n",
    "                rewards[i, action] = reward_gen.generate_rewards3(context[i], action,  optimal_actions[i])\n",
    "            elif reward_type == 'lin4':\n",
    "                rewards[i, action] = reward_gen.generate_rewards4(context[i], action,  optimal_actions[i])              \n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# Function to generate actions from a normal distribution and clip them to valid action range\n",
    "def generate_actions(total_samples):\n",
    "    return  np.random.choice([0, 1], size=total_samples)\n",
    "\n",
    "\n",
    "# Define the data generation and splitting function\n",
    "def generate_data_and_split(X, total_samples, n_actions, optimal_actions, reward_type):\n",
    "    \n",
    "    rewards = generate_data(X, total_samples, n_actions, optimal_actions, reward_type)\n",
    "    split_index = int(total_samples * 0.9)\n",
    "    X_train = X[:split_index]\n",
    "    historical_actions = generate_actions(split_index)\n",
    "    historical_rewards = rewards[:split_index]\n",
    "    \n",
    "    X_val = X[split_index:]\n",
    "    optimal_actions_val = optimal_actions[split_index:]\n",
    "    rewards_val = rewards[split_index:]\n",
    "    \n",
    "    return X_train, X_val, historical_actions, historical_rewards, optimal_actions_val, rewards_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9263e20-ffb8-429b-a32f-2c390b09289a",
   "metadata": {},
   "source": [
    "## 3. LinUCB with DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69d0bc74-0456-45df-ba8a-bc4dc668d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinUCB_DR:\n",
    "    def __init__(self, n_actions, context_dim, alpha=0.5):\n",
    "        self.n_actions = n_actions\n",
    "        self.context_dim = context_dim\n",
    "        self.alpha = alpha\n",
    "        self.A = [np.eye(context_dim) for _ in range(n_actions)]  # Identity matrices for each action\n",
    "        self.b = [np.zeros(context_dim) for _ in range(n_actions)]  # Zero vectors for each action\n",
    "        self.propensities = None\n",
    "\n",
    "    def calculate_propensity_scores(self, historical_actions):\n",
    "        # Calculate propensity scores based on historical action frequencies\n",
    "        total_samples = len(historical_actions)\n",
    "        action_freq = np.bincount(historical_actions, minlength=self.n_actions) / total_samples\n",
    "        self.propensities = action_freq[historical_actions]\n",
    "        return self.propensities\n",
    "\n",
    "    def update(self, action, reward, context):\n",
    "        # Update A and b for the taken action\n",
    "        self.A[action] += np.outer(context, context)\n",
    "        self.b[action] += reward * context\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict the reward for each action in the context X\n",
    "        p = np.zeros((X.shape[0], self.n_actions))\n",
    "        for a in range(self.n_actions):\n",
    "            theta_a = np.linalg.solve(self.A[a], self.b[a])  # Solve A * theta = b\n",
    "            p[:, a] = X @ theta_a + self.alpha * np.sqrt(np.sum(X @ np.linalg.inv(self.A[a]) * X, axis=1))\n",
    "        return p\n",
    "\n",
    "    def doubly_robust_estimator(self, X, chosen_actions, historical_rewards, historical_actions, all_true_rewards):\n",
    "        # Calculate doubly robust reward estimates\n",
    "        N = len(chosen_actions)\n",
    "        dr_rewards = np.zeros(N)\n",
    "        propensities = self.propensities if self.propensities is not None else self.calculate_propensity_scores(historical_actions)\n",
    "\n",
    "        numerator_all = np.exp(self.predict(X))\n",
    "        for i in range(N):\n",
    "            Ai = chosen_actions[i]\n",
    "            Ci = X[i]\n",
    "            Ri = historical_rewards[i, historical_actions[i]]\n",
    "            # pi_ratio = propensities[i] / propensities[chosen_actions[i]]\n",
    "            # pi_ratio = propensities[chosen_actions[i]] / propensities[historical_actions[i]]\n",
    "            \n",
    "            # numerator = p/sum(p)  (softmax)\n",
    "            # numerator = numerator_all[i] / np.sum(numerator_all[i])\n",
    "            pi_ratio = numerator_all[i, Ai] / np.sum(numerator_all[i])\n",
    "            # print(pi_ratio)\n",
    "\n",
    "            predicted_reward = all_true_rewards[i, Ai]\n",
    "            \n",
    "            # dr_rewards[i] = Ri * pi_ratio + 1 / pi_ratio * predicted_reward\n",
    "            dr_rewards[i] = pi_ratio * (Ri - predicted_reward) + predicted_reward\n",
    "\n",
    "        return dr_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ae2b21-9292-4acf-8c30-887695ee7092",
   "metadata": {},
   "source": [
    "## 4. Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b53381b-fe51-466d-bc4e-918927e648ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backtest_DR(X, total_samples, n_actions, optimal_actions, context_dim, num_samples=10, lengthscale=1.0, outputscale=1.0, epochs=20, reward_type='lin1'):\n",
    "    # Generate and split the dataset\n",
    "    X_train, X_val, historical_actions, historical_rewards, optimal_actions_val, rewards_val = generate_data_and_split(\n",
    "        X, total_samples, n_actions, optimal_actions, reward_type)\n",
    "\n",
    "    # Convert PyTorch tensors to NumPy arrays if needed\n",
    "    if isinstance(X_train, torch.Tensor):\n",
    "        X_train = X_train.detach().numpy()\n",
    "    if isinstance(X_val, torch.Tensor):\n",
    "        X_val = X_val.detach().numpy()\n",
    "    if isinstance(historical_rewards, torch.Tensor):\n",
    "        historical_rewards = historical_rewards.detach().numpy()\n",
    "    if isinstance(historical_actions, torch.Tensor):\n",
    "        historical_actions = historical_actions.detach().numpy()\n",
    "\n",
    "    # Initialize LinUCB_DR model\n",
    "    lin_UCB_DR_model = LinUCB_DR(n_actions, context_dim, alpha=0.5)\n",
    "    lin_UCB_DR_model.calculate_propensity_scores(historical_actions)  # Calculate propensities internally\n",
    "\n",
    "    # Update the model with historical data\n",
    "    for i in range(len(historical_actions)):\n",
    "        lin_UCB_DR_model.update(historical_actions[i], historical_rewards[i, historical_actions[i]], X_train[i])\n",
    "\n",
    "    # Generate predictions on validation data\n",
    "    lin_UCB_preds = lin_UCB_DR_model.predict(X_val)\n",
    "    lin_UCB_preds = np.argmax(lin_UCB_preds, axis=1)\n",
    "\n",
    "    # Generate true rewards based on optimal actions\n",
    "    all_true_rewards = np.zeros((X_val.shape[0], n_actions))\n",
    "    reward_gen = RewardGenerator()\n",
    "    \n",
    "    for action in range(n_actions):\n",
    "        for i, opt_action in enumerate(optimal_actions_val):\n",
    "            if reward_type == 'lin1':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards1(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin2':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards2(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin3':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards3(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin4':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards4(X_val[i], action, opt_action)\n",
    "\n",
    "    # Calculate doubly robust rewards\n",
    "    dr_rewards = lin_UCB_DR_model.doubly_robust_estimator(X_val, lin_UCB_preds, historical_rewards, historical_actions, all_true_rewards)\n",
    "\n",
    "    # return average rewards\n",
    "    print(f\"Average Rewards_DR: {np.mean(dr_rewards):.4f}\")\n",
    "    print(f\"LinUCB Accuracy: {accuracy_score(optimal_actions_val, lin_UCB_preds):.4f}\")\n",
    "    print(f\"LinUCB F1 Score: {f1_score(optimal_actions_val, lin_UCB_preds, average='weighted'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a464f52f-ff3b-45fe-aa78-a46d5bbf86e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "X = df2.values\n",
    "total_samples = X.shape[0]\n",
    "n_actions = 2\n",
    "context_dim = X.shape[1]   # Number of context features\n",
    "optimal_actions = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "27e53220-2795-45b0-8823-bf2f7b31bafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rewards_DR: 1.1517\n",
      "LinUCB Accuracy: 0.6100\n",
      "LinUCB F1 Score: 0.6004\n",
      "Average Rewards_DR: 1.9631\n",
      "LinUCB Accuracy: 0.5100\n",
      "LinUCB F1 Score: 0.5228\n",
      "Average Rewards_DR: 0.8326\n",
      "LinUCB Accuracy: 0.6600\n",
      "LinUCB F1 Score: 0.6600\n",
      "Average Rewards_DR: 1.6271\n",
      "LinUCB Accuracy: 0.4800\n",
      "LinUCB F1 Score: 0.4945\n"
     ]
    }
   ],
   "source": [
    "run_backtest_DR(X, total_samples, n_actions, optimal_actions, context_dim,  num_samples=10, lengthscale=1.0, outputscale=1.0, epochs=50)\n",
    "run_backtest_DR(X, total_samples, n_actions, optimal_actions, context_dim,  num_samples=10, lengthscale=1.0, outputscale=1.0, epochs=50,reward_type='lin2')\n",
    "run_backtest_DR(X, total_samples, n_actions, optimal_actions, context_dim,  num_samples=10, lengthscale=1.0, outputscale=1.0, epochs=50,reward_type='lin3')\n",
    "run_backtest_DR(X, total_samples, n_actions, optimal_actions, context_dim,  num_samples=10, lengthscale=1.0, outputscale=1.0, epochs=50,reward_type='lin4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3410f7a-4a5c-4f2f-8619-7f4de5327b5b",
   "metadata": {},
   "source": [
    "## 5. Different alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20a397aa-b25e-409b-9b9d-0db4075374d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_backtest_with_alpha(X, total_samples, n_actions, optimal_actions, context_dim, reward_type='linear', alpha_values=[0.1, 0.5, 1.0], epochs=20):\n",
    "#     X_train, X_val, historical_actions, historical_rewards, optimal_actions_val, rewards_val = generate_data_and_split(\n",
    "#         X, total_samples, n_actions, optimal_actions, reward_type)\n",
    "\n",
    "#     # Convert PyTorch tensors to NumPy arrays if needed\n",
    "#     if isinstance(X_train, torch.Tensor):\n",
    "#         X_train = X_train.detach().numpy()\n",
    "#     if isinstance(X_val, torch.Tensor):\n",
    "#         X_val = X_val.detach().numpy()\n",
    "#     if isinstance(historical_rewards, torch.Tensor):\n",
    "#         historical_rewards = historical_rewards.detach().numpy()\n",
    "#     if isinstance(historical_actions, torch.Tensor):\n",
    "#         historical_actions = historical_actions.detach().numpy()\n",
    "\n",
    "#     # Initialize LinUCB_DR model\n",
    "#     lin_UCB_DR_model = LinUCB_DR(n_actions, context_dim, alpha=0.5)\n",
    "#     lin_UCB_DR_model.calculate_propensity_scores(historical_actions)  # Calculate propensities internally\n",
    "\n",
    "#     # Update the model with historical data\n",
    "#     for i in range(len(historical_actions)):\n",
    "#         lin_UCB_DR_model.update(historical_actions[i], historical_rewards[i, historical_actions[i]], X_train[i])\n",
    "\n",
    "#     # Generate predictions on validation data\n",
    "#     lin_UCB_preds = lin_UCB_DR_model.predict(X_val)\n",
    "#     lin_UCB_preds = np.argmax(lin_UCB_preds, axis=1)\n",
    "\n",
    "#     # Generate true rewards based on optimal actions\n",
    "#     all_true_rewards = np.zeros((X_val.shape[0], n_actions))\n",
    "#     reward_gen = RewardGenerator()\n",
    "    \n",
    "#     for action in range(n_actions):\n",
    "#         for i, opt_action in enumerate(optimal_actions_val):\n",
    "#             if reward_type == 'lin1':\n",
    "#                 all_true_rewards[i, action] = reward_gen.generate_rewards1(X_val[i], action, opt_action)\n",
    "#             elif reward_type == 'lin2':\n",
    "#                 all_true_rewards[i, action] = reward_gen.generate_rewards2(X_val[i], action, opt_action)\n",
    "#             elif reward_type == 'lin3':\n",
    "#                 all_true_rewards[i, action] = reward_gen.generate_rewards3(X_val[i], action, opt_action)\n",
    "#             elif reward_type == 'lin4':\n",
    "#                 all_true_rewards[i, action] = reward_gen.generate_rewards4(X_val[i], action, opt_action)\n",
    "\n",
    "#     # Calculate doubly robust rewards\n",
    "#     dr_rewards = lin_UCB_DR_model.doubly_robust_estimator(X_val, lin_UCB_preds, historical_rewards, historical_actions, all_true_rewards)\n",
    "    \n",
    "#     for alpha in alpha_values:\n",
    "#         # LinUCB with different alphas\n",
    "#         lin_UCB_model = LinUCB_DR(n_actions, context_dim, alpha=alpha)\n",
    "        \n",
    "#         # Train LinUCB model\n",
    "#         for i in range(len(historical_actions)):\n",
    "#             lin_UCB_model.update(historical_actions[i], historical_rewards[i, historical_actions[i]], X_train[i])\n",
    "        \n",
    "#         # Predict with LinUCB on validation set\n",
    "#         lin_UCB_preds = lin_UCB_model.predict(X_val)\n",
    "#         lin_UCB_preds = np.argmax(lin_UCB_preds, axis=1)\n",
    "        \n",
    "#         # Calculate rewards from LinUCB strategy\n",
    "#         lin_UCB_rewards = np.array([all_true_rewards[i, lin_UCB_preds[i]] for i in range(len(lin_UCB_preds))])\n",
    "\n",
    "        \n",
    "#         # return average rewards\n",
    "#         print(f\"Alpha: {alpha}, Average Rewards_DR: {np.mean(lin_UCB_rewards):.4f}\")\n",
    "#         #print(f\"LinUCB Accuracy: {accuracy_score(optimal_actions_val, lin_UCB_preds):.4f}\")\n",
    "#         #print(f\"LinUCB F1 Score: {f1_score(optimal_actions_val, lin_UCB_preds, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f0183950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backtest_with_alpha(X, total_samples, n_actions, optimal_actions, context_dim, reward_type='linear', alpha_values=[0.1, 0.5, 1.0], epochs=20):\n",
    "    X_train, X_val, historical_actions, historical_rewards, optimal_actions_val, rewards_val = generate_data_and_split(\n",
    "        X, total_samples, n_actions, optimal_actions, reward_type)\n",
    "\n",
    "    # Convert PyTorch tensors to NumPy arrays if needed\n",
    "    if isinstance(X_train, torch.Tensor):\n",
    "        X_train = X_train.detach().numpy()\n",
    "    if isinstance(X_val, torch.Tensor):\n",
    "        X_val = X_val.detach().numpy()\n",
    "    if isinstance(historical_rewards, torch.Tensor):\n",
    "        historical_rewards = historical_rewards.detach().numpy()\n",
    "    if isinstance(historical_actions, torch.Tensor):\n",
    "        historical_actions = historical_actions.detach().numpy()\n",
    "\n",
    "    # Generate true rewards based on optimal actions\n",
    "    all_true_rewards = np.zeros((X_val.shape[0], n_actions))\n",
    "    reward_gen = RewardGenerator()\n",
    "    \n",
    "    for action in range(n_actions):\n",
    "        for i, opt_action in enumerate(optimal_actions_val):\n",
    "            if reward_type == 'lin1':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards1(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin2':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards2(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin3':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards3(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin4':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards4(X_val[i], action, opt_action)\n",
    "\n",
    "    # Loop over alpha values and calculate DR rewards\n",
    "    for alpha in alpha_values:\n",
    "        # Initialize LinUCB_DR model with the current alpha\n",
    "        lin_UCB_DR_model = LinUCB_DR(n_actions, context_dim, alpha=alpha)\n",
    "        lin_UCB_DR_model.calculate_propensity_scores(historical_actions)  # Calculate propensities internally\n",
    "        \n",
    "        # Update the model with historical data\n",
    "        for i in range(len(historical_actions)):\n",
    "            lin_UCB_DR_model.update(historical_actions[i], historical_rewards[i, historical_actions[i]], X_train[i])\n",
    "\n",
    "        # Generate predictions on validation data\n",
    "        lin_UCB_preds = lin_UCB_DR_model.predict(X_val)\n",
    "        lin_UCB_preds = np.argmax(lin_UCB_preds, axis=1)\n",
    "\n",
    "        # Calculate doubly robust rewards\n",
    "        dr_rewards = lin_UCB_DR_model.doubly_robust_estimator(X_val, lin_UCB_preds, historical_rewards, historical_actions, all_true_rewards)\n",
    "        \n",
    "        # Output average DR rewards for the current alpha\n",
    "        print(f\"Alpha: {alpha}, Average DR Rewards: {np.mean(dr_rewards):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db1f0cd2-2b00-487b-8cbc-18f70d397977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Average DR Rewards: 1.1361\n",
      "Alpha: 0.1, Average DR Rewards: 1.1361\n",
      "Alpha: 1.0, Average DR Rewards: 1.1361\n",
      "Alpha: 2.0, Average DR Rewards: 1.1390\n",
      "Alpha: 5.0, Average DR Rewards: 1.1318\n",
      "Alpha: 10.0, Average DR Rewards: 1.1367\n",
      "Alpha: 15.0, Average DR Rewards: 1.1342\n",
      "Alpha: 20.0, Average DR Rewards: 1.1310\n",
      "Alpha: 50.0, Average DR Rewards: 1.1186\n"
     ]
    }
   ],
   "source": [
    "alpha_values = [0.01, 0.1, 1.0, 2.0, 5.0, 10.0, 15.0, 20.0, 50.0]\n",
    "run_backtest_with_alpha(X, total_samples=1000, n_actions=2, optimal_actions=optimal_actions, context_dim=X.shape[1], reward_type='lin1', alpha_values=alpha_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df51ae6f-4ed6-4b52-a9e0-b7c6e6542141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Average DR Rewards: 2.0586\n",
      "Alpha: 0.1, Average DR Rewards: 2.0586\n",
      "Alpha: 1.0, Average DR Rewards: 2.0586\n",
      "Alpha: 2.0, Average DR Rewards: 2.0586\n",
      "Alpha: 5.0, Average DR Rewards: 2.0674\n",
      "Alpha: 10.0, Average DR Rewards: 2.0708\n",
      "Alpha: 15.0, Average DR Rewards: 2.0722\n",
      "Alpha: 20.0, Average DR Rewards: 2.0636\n",
      "Alpha: 50.0, Average DR Rewards: 2.0429\n"
     ]
    }
   ],
   "source": [
    "run_backtest_with_alpha(X, total_samples=1000, n_actions=2, optimal_actions=optimal_actions, context_dim=X.shape[1], reward_type='lin2', alpha_values=alpha_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e423edb-b328-4f43-ae0b-b02418730f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Average DR Rewards: 0.7570\n",
      "Alpha: 0.1, Average DR Rewards: 0.7570\n",
      "Alpha: 1.0, Average DR Rewards: 0.7570\n",
      "Alpha: 2.0, Average DR Rewards: 0.7571\n",
      "Alpha: 5.0, Average DR Rewards: 0.7497\n",
      "Alpha: 10.0, Average DR Rewards: 0.7591\n",
      "Alpha: 15.0, Average DR Rewards: 0.7747\n",
      "Alpha: 20.0, Average DR Rewards: 0.7715\n",
      "Alpha: 50.0, Average DR Rewards: 0.7210\n"
     ]
    }
   ],
   "source": [
    "run_backtest_with_alpha(X, total_samples=1000, n_actions=2, optimal_actions=optimal_actions, context_dim=X.shape[1], reward_type='lin3', alpha_values=alpha_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "95f7a861-e3c5-411e-99b0-8cc5670caafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Average DR Rewards: 1.6378\n",
      "Alpha: 0.1, Average DR Rewards: 1.6378\n",
      "Alpha: 1.0, Average DR Rewards: 1.6452\n",
      "Alpha: 2.0, Average DR Rewards: 1.6322\n",
      "Alpha: 5.0, Average DR Rewards: 1.6281\n",
      "Alpha: 10.0, Average DR Rewards: 1.6395\n",
      "Alpha: 15.0, Average DR Rewards: 1.6385\n",
      "Alpha: 20.0, Average DR Rewards: 1.6330\n",
      "Alpha: 50.0, Average DR Rewards: 1.6378\n"
     ]
    }
   ],
   "source": [
    "run_backtest_with_alpha(X, total_samples=1000, n_actions=2, optimal_actions=optimal_actions, context_dim=X.shape[1], reward_type='lin4', alpha_values=alpha_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc61737c",
   "metadata": {},
   "source": [
    "## 6. Statistical Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ccd4de22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_backtest_with_alpha_test(X, total_samples, n_actions, optimal_actions, context_dim, reward_type='linear', alpha_values=[0.1, 0.5, 1.0], epochs=20):\n",
    "#     X_train, X_val, historical_actions, historical_rewards, optimal_actions_val, rewards_val = generate_data_and_split(\n",
    "#         X, total_samples, n_actions, optimal_actions, reward_type)\n",
    "\n",
    "#     # Convert PyTorch tensors to NumPy arrays if needed\n",
    "#     if isinstance(X_train, torch.Tensor):\n",
    "#         X_train = X_train.detach().numpy()\n",
    "#     if isinstance(X_val, torch.Tensor):\n",
    "#         X_val = X_val.detach().numpy()\n",
    "#     if isinstance(historical_rewards, torch.Tensor):\n",
    "#         historical_rewards = historical_rewards.detach().numpy()\n",
    "#     if isinstance(historical_actions, torch.Tensor):\n",
    "#         historical_actions = historical_actions.detach().numpy()\n",
    "\n",
    "#     # Initialize LinUCB_DR model\n",
    "#     lin_UCB_DR_model = LinUCB_DR(n_actions, context_dim, alpha=0.5)\n",
    "#     lin_UCB_DR_model.calculate_propensity_scores(historical_actions)  # Calculate propensities internally\n",
    "\n",
    "#     # Update the model with historical data\n",
    "#     for i in range(len(historical_actions)):\n",
    "#         lin_UCB_DR_model.update(historical_actions[i], historical_rewards[i, historical_actions[i]], X_train[i])\n",
    "\n",
    "#     # Generate predictions on validation data\n",
    "#     lin_UCB_preds = lin_UCB_DR_model.predict(X_val)\n",
    "#     lin_UCB_preds = np.argmax(lin_UCB_preds, axis=1)\n",
    "\n",
    "#     # Generate true rewards based on optimal actions\n",
    "#     all_true_rewards = np.zeros((X_val.shape[0], n_actions))\n",
    "#     reward_gen = RewardGenerator()\n",
    "    \n",
    "#     for action in range(n_actions):\n",
    "#         for i, opt_action in enumerate(optimal_actions_val):\n",
    "#             if reward_type == 'lin1':\n",
    "#                 all_true_rewards[i, action] = reward_gen.generate_rewards1(X_val[i], action, opt_action)\n",
    "#             elif reward_type == 'lin2':\n",
    "#                 all_true_rewards[i, action] = reward_gen.generate_rewards2(X_val[i], action, opt_action)\n",
    "#             elif reward_type == 'lin3':\n",
    "#                 all_true_rewards[i, action] = reward_gen.generate_rewards3(X_val[i], action, opt_action)\n",
    "#             elif reward_type == 'lin4':\n",
    "#                 all_true_rewards[i, action] = reward_gen.generate_rewards4(X_val[i], action, opt_action)\n",
    "\n",
    "#     # Calculate doubly robust rewards\n",
    "#     dr_rewards = lin_UCB_DR_model.doubly_robust_estimator(X_val, lin_UCB_preds, historical_rewards, historical_actions, all_true_rewards)\n",
    "    \n",
    "#     # Create an empty DataFrame to store results\n",
    "#     results_df = pd.DataFrame(columns=['Reward Type', 'Alpha', 'Avg DR Rewards', 'Avg LinUCB Rewards', 'Paired t-stat', 'Paired p-value', 'Observed Mean Diff', 'Bootstrap p-value'])\n",
    "    \n",
    "#     for alpha in alpha_values:\n",
    "#         # LinUCB with different alphas\n",
    "#         lin_UCB_model = LinUCB_DR(n_actions, context_dim, alpha=alpha)\n",
    "        \n",
    "#         # Train LinUCB model\n",
    "#         for i in range(len(historical_actions)):\n",
    "#             lin_UCB_model.update(historical_actions[i], historical_rewards[i, historical_actions[i]], X_train[i])\n",
    "        \n",
    "#         # Predict with LinUCB on validation set\n",
    "#         lin_UCB_preds = lin_UCB_model.predict(X_val)\n",
    "#         lin_UCB_preds = np.argmax(lin_UCB_preds, axis=1)\n",
    "        \n",
    "#         # Calculate rewards from LinUCB strategy\n",
    "#         lin_UCB_rewards = np.array([all_true_rewards[i, lin_UCB_preds[i]] for i in range(len(lin_UCB_preds))])\n",
    "#         lin_UCB_rewards_avg = np.mean(lin_UCB_rewards)\n",
    "#         dr_rewards_avg = np.mean(dr_rewards)\n",
    "        \n",
    "#         # Paired t-test between DR rewards and each LinUCB with alpha\n",
    "#         t_stat, p_value_ttest = stats.ttest_rel(dr_rewards, lin_UCB_rewards)\n",
    "\n",
    "#         # Bootstrap Test\n",
    "#         n_bootstrap = 10000\n",
    "#         differences = dr_rewards - lin_UCB_rewards\n",
    "#         observed_mean_diff = np.mean(differences)\n",
    "        \n",
    "#         # Bootstrap sampling\n",
    "#         bootstrap_means = np.array([\n",
    "#             np.mean(np.random.choice(differences, size=len(differences), replace=True)) \n",
    "#             for _ in range(n_bootstrap)\n",
    "#         ])\n",
    "        \n",
    "#         # Calculate the p-value as the proportion of bootstrap samples with a mean greater than or equal to the observed mean difference\n",
    "#         p_value_bootstrap = np.mean(bootstrap_means >= observed_mean_diff)\n",
    "        \n",
    "#         # Append results to the DataFrame\n",
    "#         results_df = results_df.append({\n",
    "#             'Reward Type': reward_type,\n",
    "#             'Alpha': alpha,\n",
    "#             'Avg DR Rewards': dr_rewards_avg,\n",
    "#             'Avg LinUCB Rewards': lin_UCB_rewards_avg,\n",
    "#             'Paired t-stat': t_stat,\n",
    "#             'Paired p-value': p_value_ttest,\n",
    "#             'Observed Mean Diff': observed_mean_diff,\n",
    "#             'Bootstrap p-value': p_value_bootstrap\n",
    "#         }, ignore_index=True)\n",
    "\n",
    "#     return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bca97a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backtest_with_alpha_test(X, total_samples, n_actions, optimal_actions, context_dim, reward_type='linear', alpha_values=[0.1, 0.5, 1.0], epochs=20):\n",
    "    X_train, X_val, historical_actions, historical_rewards, optimal_actions_val, rewards_val = generate_data_and_split(\n",
    "        X, total_samples, n_actions, optimal_actions, reward_type)\n",
    "\n",
    "    # Convert PyTorch tensors to NumPy arrays if needed\n",
    "    if isinstance(X_train, torch.Tensor):\n",
    "        X_train = X_train.detach().numpy()\n",
    "    if isinstance(X_val, torch.Tensor):\n",
    "        X_val = X_val.detach().numpy()\n",
    "    if isinstance(historical_rewards, torch.Tensor):\n",
    "        historical_rewards = historical_rewards.detach().numpy()\n",
    "    if isinstance(historical_actions, torch.Tensor):\n",
    "        historical_actions = historical_actions.detach().numpy()\n",
    "\n",
    "    # Generate true rewards based on optimal actions\n",
    "    all_true_rewards = np.zeros((X_val.shape[0], n_actions))\n",
    "    reward_gen = RewardGenerator()\n",
    "    \n",
    "    for action in range(n_actions):\n",
    "        for i, opt_action in enumerate(optimal_actions_val):\n",
    "            if reward_type == 'lin1':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards1(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin2':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards2(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin3':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards3(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin4':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards4(X_val[i], action, opt_action)\n",
    "\n",
    "    # Create an empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['Reward Type', 'Alpha', 'Avg DR Rewards', 'Avg LinUCB Rewards', 'Paired t-stat', 'Paired p-value', 'Observed Mean Diff', 'Bootstrap p-value'])\n",
    "    \n",
    "    for alpha in alpha_values:\n",
    "        # Initialize LinUCB_DR model with the current alpha\n",
    "        lin_UCB_DR_model = LinUCB_DR(n_actions, context_dim, alpha=alpha)\n",
    "        lin_UCB_DR_model.calculate_propensity_scores(historical_actions)  # Calculate propensities internally\n",
    "        \n",
    "        # Train LinUCB model\n",
    "        for i in range(len(historical_actions)):\n",
    "            lin_UCB_DR_model.update(historical_actions[i], historical_rewards[i, historical_actions[i]], X_train[i])\n",
    "        \n",
    "        # Generate predictions on validation data\n",
    "        lin_UCB_preds = lin_UCB_DR_model.predict(X_val)\n",
    "        lin_UCB_preds = np.argmax(lin_UCB_preds, axis=1)\n",
    "\n",
    "        # Calculate doubly robust rewards\n",
    "        dr_rewards = lin_UCB_DR_model.doubly_robust_estimator(X_val, lin_UCB_preds, historical_rewards, historical_actions, all_true_rewards)\n",
    "        dr_rewards_avg = np.mean(dr_rewards)\n",
    "        \n",
    "        # Calculate rewards from LinUCB strategy\n",
    "        lin_UCB_rewards = np.array([all_true_rewards[i, lin_UCB_preds[i]] for i in range(len(lin_UCB_preds))])\n",
    "        lin_UCB_rewards_avg = np.mean(lin_UCB_rewards)\n",
    "        \n",
    "        # Paired t-test between DR rewards and each LinUCB with alpha\n",
    "        t_stat, p_value_ttest = stats.ttest_rel(dr_rewards, lin_UCB_rewards)\n",
    "\n",
    "        # Bootstrap Test\n",
    "        n_bootstrap = 10000\n",
    "        differences = dr_rewards - lin_UCB_rewards\n",
    "        observed_mean_diff = np.mean(differences)\n",
    "        \n",
    "        # Bootstrap sampling\n",
    "        bootstrap_means = np.array([\n",
    "            np.mean(np.random.choice(differences, size=len(differences), replace=True)) \n",
    "            for _ in range(n_bootstrap)\n",
    "        ])\n",
    "        \n",
    "        # Calculate the p-value as the proportion of bootstrap samples with a mean greater than or equal to the observed mean difference\n",
    "        p_value_bootstrap = np.mean(bootstrap_means >= observed_mean_diff)\n",
    "        \n",
    "        # Append results to the DataFrame\n",
    "        results_df = results_df.append({\n",
    "            'Reward Type': reward_type,\n",
    "            'Alpha': alpha,\n",
    "            'Avg DR Rewards': dr_rewards_avg,\n",
    "            'Avg LinUCB Rewards': lin_UCB_rewards_avg,\n",
    "            'Paired t-stat': t_stat,\n",
    "            'Paired p-value': p_value_ttest,\n",
    "            'Observed Mean Diff': observed_mean_diff,\n",
    "            'Bootstrap p-value': p_value_bootstrap\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87e0ec25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reward Type</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Avg DR Rewards</th>\n",
       "      <th>Avg LinUCB Rewards</th>\n",
       "      <th>Paired t-stat</th>\n",
       "      <th>Paired p-value</th>\n",
       "      <th>Observed Mean Diff</th>\n",
       "      <th>Bootstrap p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lin1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.16926</td>\n",
       "      <td>1.286153</td>\n",
       "      <td>-1.086068</td>\n",
       "      <td>0.280086</td>\n",
       "      <td>-0.116893</td>\n",
       "      <td>0.5022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lin1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.169355</td>\n",
       "      <td>1.286153</td>\n",
       "      <td>-1.085239</td>\n",
       "      <td>0.280451</td>\n",
       "      <td>-0.116798</td>\n",
       "      <td>0.5042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lin1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.170309</td>\n",
       "      <td>1.286153</td>\n",
       "      <td>-1.076954</td>\n",
       "      <td>0.284118</td>\n",
       "      <td>-0.115845</td>\n",
       "      <td>0.5027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lin1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.171368</td>\n",
       "      <td>1.286153</td>\n",
       "      <td>-1.067759</td>\n",
       "      <td>0.288226</td>\n",
       "      <td>-0.114785</td>\n",
       "      <td>0.5061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lin1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.174544</td>\n",
       "      <td>1.286153</td>\n",
       "      <td>-1.040214</td>\n",
       "      <td>0.300774</td>\n",
       "      <td>-0.111609</td>\n",
       "      <td>0.4961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lin1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.128297</td>\n",
       "      <td>1.186552</td>\n",
       "      <td>-0.551012</td>\n",
       "      <td>0.582867</td>\n",
       "      <td>-0.058255</td>\n",
       "      <td>0.4916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lin1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.138332</td>\n",
       "      <td>1.203189</td>\n",
       "      <td>-0.611722</td>\n",
       "      <td>0.542124</td>\n",
       "      <td>-0.064857</td>\n",
       "      <td>0.4955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lin1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.138725</td>\n",
       "      <td>1.203104</td>\n",
       "      <td>-0.606735</td>\n",
       "      <td>0.545416</td>\n",
       "      <td>-0.064379</td>\n",
       "      <td>0.4969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lin1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.159848</td>\n",
       "      <td>1.214703</td>\n",
       "      <td>-0.498635</td>\n",
       "      <td>0.619142</td>\n",
       "      <td>-0.054855</td>\n",
       "      <td>0.5052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lin2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.102271</td>\n",
       "      <td>2.071171</td>\n",
       "      <td>0.556737</td>\n",
       "      <td>0.578963</td>\n",
       "      <td>0.0311</td>\n",
       "      <td>0.5001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lin2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.10215</td>\n",
       "      <td>2.071171</td>\n",
       "      <td>0.554421</td>\n",
       "      <td>0.580541</td>\n",
       "      <td>0.030978</td>\n",
       "      <td>0.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lin2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.100942</td>\n",
       "      <td>2.071171</td>\n",
       "      <td>0.531508</td>\n",
       "      <td>0.596257</td>\n",
       "      <td>0.029771</td>\n",
       "      <td>0.4972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lin2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.109171</td>\n",
       "      <td>2.090259</td>\n",
       "      <td>0.336845</td>\n",
       "      <td>0.736947</td>\n",
       "      <td>0.018912</td>\n",
       "      <td>0.5017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lin2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.116131</td>\n",
       "      <td>2.11162</td>\n",
       "      <td>0.07968</td>\n",
       "      <td>0.936653</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>0.4928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lin2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.130708</td>\n",
       "      <td>2.151686</td>\n",
       "      <td>-0.367897</td>\n",
       "      <td>0.713736</td>\n",
       "      <td>-0.020978</td>\n",
       "      <td>0.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lin2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.12211</td>\n",
       "      <td>2.142949</td>\n",
       "      <td>-0.357819</td>\n",
       "      <td>0.721241</td>\n",
       "      <td>-0.020838</td>\n",
       "      <td>0.4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>lin2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.124546</td>\n",
       "      <td>2.155377</td>\n",
       "      <td>-0.522231</td>\n",
       "      <td>0.602676</td>\n",
       "      <td>-0.030831</td>\n",
       "      <td>0.5055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lin2</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.087798</td>\n",
       "      <td>2.120578</td>\n",
       "      <td>-0.516175</td>\n",
       "      <td>0.606883</td>\n",
       "      <td>-0.03278</td>\n",
       "      <td>0.5039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lin3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.888074</td>\n",
       "      <td>0.838481</td>\n",
       "      <td>0.278393</td>\n",
       "      <td>0.781291</td>\n",
       "      <td>0.049593</td>\n",
       "      <td>0.4967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lin3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.888001</td>\n",
       "      <td>0.838481</td>\n",
       "      <td>0.278055</td>\n",
       "      <td>0.78155</td>\n",
       "      <td>0.049519</td>\n",
       "      <td>0.5034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lin3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.887268</td>\n",
       "      <td>0.838481</td>\n",
       "      <td>0.274667</td>\n",
       "      <td>0.784144</td>\n",
       "      <td>0.048787</td>\n",
       "      <td>0.4942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>lin3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.886459</td>\n",
       "      <td>0.838481</td>\n",
       "      <td>0.270905</td>\n",
       "      <td>0.787028</td>\n",
       "      <td>0.047977</td>\n",
       "      <td>0.5012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>lin3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.880496</td>\n",
       "      <td>0.836318</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.800793</td>\n",
       "      <td>0.044178</td>\n",
       "      <td>0.4973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lin3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.929551</td>\n",
       "      <td>0.942412</td>\n",
       "      <td>-0.075303</td>\n",
       "      <td>0.940126</td>\n",
       "      <td>-0.012861</td>\n",
       "      <td>0.4965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>lin3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.877939</td>\n",
       "      <td>0.843297</td>\n",
       "      <td>0.20222</td>\n",
       "      <td>0.840159</td>\n",
       "      <td>0.034641</td>\n",
       "      <td>0.4908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>lin3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.868083</td>\n",
       "      <td>0.82803</td>\n",
       "      <td>0.22954</td>\n",
       "      <td>0.818923</td>\n",
       "      <td>0.040053</td>\n",
       "      <td>0.5002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>lin3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.85702</td>\n",
       "      <td>0.783649</td>\n",
       "      <td>0.408564</td>\n",
       "      <td>0.683742</td>\n",
       "      <td>0.073371</td>\n",
       "      <td>0.496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>lin4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.578551</td>\n",
       "      <td>1.545215</td>\n",
       "      <td>0.537997</td>\n",
       "      <td>0.591787</td>\n",
       "      <td>0.033337</td>\n",
       "      <td>0.5099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>lin4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.57863</td>\n",
       "      <td>1.545215</td>\n",
       "      <td>0.539382</td>\n",
       "      <td>0.590834</td>\n",
       "      <td>0.033416</td>\n",
       "      <td>0.5049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lin4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.58319</td>\n",
       "      <td>1.552087</td>\n",
       "      <td>0.506054</td>\n",
       "      <td>0.613944</td>\n",
       "      <td>0.031103</td>\n",
       "      <td>0.5061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>lin4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.584778</td>\n",
       "      <td>1.552087</td>\n",
       "      <td>0.532758</td>\n",
       "      <td>0.595395</td>\n",
       "      <td>0.032691</td>\n",
       "      <td>0.4998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>lin4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.593198</td>\n",
       "      <td>1.558267</td>\n",
       "      <td>0.573104</td>\n",
       "      <td>0.567874</td>\n",
       "      <td>0.034931</td>\n",
       "      <td>0.5138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>lin4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.591903</td>\n",
       "      <td>1.542176</td>\n",
       "      <td>0.829809</td>\n",
       "      <td>0.408643</td>\n",
       "      <td>0.049727</td>\n",
       "      <td>0.5114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>lin4</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.59074</td>\n",
       "      <td>1.534445</td>\n",
       "      <td>0.936177</td>\n",
       "      <td>0.35146</td>\n",
       "      <td>0.056295</td>\n",
       "      <td>0.5047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>lin4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.607209</td>\n",
       "      <td>1.562524</td>\n",
       "      <td>0.741114</td>\n",
       "      <td>0.460379</td>\n",
       "      <td>0.044685</td>\n",
       "      <td>0.5039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>lin4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.608078</td>\n",
       "      <td>1.557321</td>\n",
       "      <td>0.807348</td>\n",
       "      <td>0.421402</td>\n",
       "      <td>0.050757</td>\n",
       "      <td>0.5217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Reward Type Alpha Avg DR Rewards Avg LinUCB Rewards Paired t-stat  \\\n",
       "0         lin1  0.01        1.16926           1.286153     -1.086068   \n",
       "1         lin1   0.1       1.169355           1.286153     -1.085239   \n",
       "2         lin1   1.0       1.170309           1.286153     -1.076954   \n",
       "3         lin1   2.0       1.171368           1.286153     -1.067759   \n",
       "4         lin1   5.0       1.174544           1.286153     -1.040214   \n",
       "5         lin1  10.0       1.128297           1.186552     -0.551012   \n",
       "6         lin1  15.0       1.138332           1.203189     -0.611722   \n",
       "7         lin1  20.0       1.138725           1.203104     -0.606735   \n",
       "8         lin1  50.0       1.159848           1.214703     -0.498635   \n",
       "9         lin2  0.01       2.102271           2.071171      0.556737   \n",
       "10        lin2   0.1        2.10215           2.071171      0.554421   \n",
       "11        lin2   1.0       2.100942           2.071171      0.531508   \n",
       "12        lin2   2.0       2.109171           2.090259      0.336845   \n",
       "13        lin2   5.0       2.116131            2.11162       0.07968   \n",
       "14        lin2  10.0       2.130708           2.151686     -0.367897   \n",
       "15        lin2  15.0        2.12211           2.142949     -0.357819   \n",
       "16        lin2  20.0       2.124546           2.155377     -0.522231   \n",
       "17        lin2  50.0       2.087798           2.120578     -0.516175   \n",
       "18        lin3  0.01       0.888074           0.838481      0.278393   \n",
       "19        lin3   0.1       0.888001           0.838481      0.278055   \n",
       "20        lin3   1.0       0.887268           0.838481      0.274667   \n",
       "21        lin3   2.0       0.886459           0.838481      0.270905   \n",
       "22        lin3   5.0       0.880496           0.836318         0.253   \n",
       "23        lin3  10.0       0.929551           0.942412     -0.075303   \n",
       "24        lin3  15.0       0.877939           0.843297       0.20222   \n",
       "25        lin3  20.0       0.868083            0.82803       0.22954   \n",
       "26        lin3  50.0        0.85702           0.783649      0.408564   \n",
       "27        lin4  0.01       1.578551           1.545215      0.537997   \n",
       "28        lin4   0.1        1.57863           1.545215      0.539382   \n",
       "29        lin4   1.0        1.58319           1.552087      0.506054   \n",
       "30        lin4   2.0       1.584778           1.552087      0.532758   \n",
       "31        lin4   5.0       1.593198           1.558267      0.573104   \n",
       "32        lin4  10.0       1.591903           1.542176      0.829809   \n",
       "33        lin4  15.0        1.59074           1.534445      0.936177   \n",
       "34        lin4  20.0       1.607209           1.562524      0.741114   \n",
       "35        lin4  50.0       1.608078           1.557321      0.807348   \n",
       "\n",
       "   Paired p-value Observed Mean Diff Bootstrap p-value  \n",
       "0        0.280086          -0.116893            0.5022  \n",
       "1        0.280451          -0.116798            0.5042  \n",
       "2        0.284118          -0.115845            0.5027  \n",
       "3        0.288226          -0.114785            0.5061  \n",
       "4        0.300774          -0.111609            0.4961  \n",
       "5        0.582867          -0.058255            0.4916  \n",
       "6        0.542124          -0.064857            0.4955  \n",
       "7        0.545416          -0.064379            0.4969  \n",
       "8        0.619142          -0.054855            0.5052  \n",
       "9        0.578963             0.0311            0.5001  \n",
       "10       0.580541           0.030978             0.502  \n",
       "11       0.596257           0.029771            0.4972  \n",
       "12       0.736947           0.018912            0.5017  \n",
       "13       0.936653           0.004511            0.4928  \n",
       "14       0.713736          -0.020978             0.502  \n",
       "15       0.721241          -0.020838            0.4912  \n",
       "16       0.602676          -0.030831            0.5055  \n",
       "17       0.606883           -0.03278            0.5039  \n",
       "18       0.781291           0.049593            0.4967  \n",
       "19        0.78155           0.049519            0.5034  \n",
       "20       0.784144           0.048787            0.4942  \n",
       "21       0.787028           0.047977            0.5012  \n",
       "22       0.800793           0.044178            0.4973  \n",
       "23       0.940126          -0.012861            0.4965  \n",
       "24       0.840159           0.034641            0.4908  \n",
       "25       0.818923           0.040053            0.5002  \n",
       "26       0.683742           0.073371             0.496  \n",
       "27       0.591787           0.033337            0.5099  \n",
       "28       0.590834           0.033416            0.5049  \n",
       "29       0.613944           0.031103            0.5061  \n",
       "30       0.595395           0.032691            0.4998  \n",
       "31       0.567874           0.034931            0.5138  \n",
       "32       0.408643           0.049727            0.5114  \n",
       "33        0.35146           0.056295            0.5047  \n",
       "34       0.460379           0.044685            0.5039  \n",
       "35       0.421402           0.050757            0.5217  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the function with a loop over different reward types and alpha values\n",
    "alpha_values = [0.01, 0.1, 1.0, 2.0, 5.0, 10.0, 15.0, 20.0, 50.0]\n",
    "# alpha_values = [1.0, 10.0, 50.0]\n",
    "results_all = pd.DataFrame()\n",
    "for reward_type in ['lin1', 'lin2', 'lin3', 'lin4']:\n",
    "    result_df = run_backtest_with_alpha_test(X, total_samples=1000, n_actions=2, optimal_actions=optimal_actions, context_dim=X.shape[1], reward_type=reward_type, alpha_values=alpha_values)\n",
    "    results_all = pd.concat([results_all, result_df], ignore_index=True)\n",
    "\n",
    "# Display the final results DataFrame\n",
    "results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0163c502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reward Type</th>\n",
       "      <th>Alpha Pair</th>\n",
       "      <th>Avg DR Rewards (No Exploration)</th>\n",
       "      <th>Avg DR Rewards (With Exploration)</th>\n",
       "      <th>Paired t-stat</th>\n",
       "      <th>Paired p-value</th>\n",
       "      <th>Observed Mean Diff</th>\n",
       "      <th>Bootstrap p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lin1</td>\n",
       "      <td>0.01 vs 5.0</td>\n",
       "      <td>1.009613</td>\n",
       "      <td>0.998694</td>\n",
       "      <td>0.874752</td>\n",
       "      <td>0.383826</td>\n",
       "      <td>-0.010919</td>\n",
       "      <td>0.5477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lin1</td>\n",
       "      <td>0.01 vs 20.0</td>\n",
       "      <td>1.009613</td>\n",
       "      <td>0.982926</td>\n",
       "      <td>1.047003</td>\n",
       "      <td>0.297648</td>\n",
       "      <td>-0.026687</td>\n",
       "      <td>0.514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lin1</td>\n",
       "      <td>0.01 vs 50.0</td>\n",
       "      <td>1.009613</td>\n",
       "      <td>0.976413</td>\n",
       "      <td>1.088175</td>\n",
       "      <td>0.27916</td>\n",
       "      <td>-0.0332</td>\n",
       "      <td>0.5081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lin2</td>\n",
       "      <td>0.01 vs 5.0</td>\n",
       "      <td>2.062473</td>\n",
       "      <td>2.065524</td>\n",
       "      <td>-0.290891</td>\n",
       "      <td>0.771744</td>\n",
       "      <td>0.003052</td>\n",
       "      <td>0.5176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lin2</td>\n",
       "      <td>0.01 vs 20.0</td>\n",
       "      <td>2.062473</td>\n",
       "      <td>2.085374</td>\n",
       "      <td>-1.981935</td>\n",
       "      <td>0.050259</td>\n",
       "      <td>0.022901</td>\n",
       "      <td>0.4967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lin2</td>\n",
       "      <td>0.01 vs 50.0</td>\n",
       "      <td>2.062473</td>\n",
       "      <td>2.103124</td>\n",
       "      <td>-2.908459</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>0.040652</td>\n",
       "      <td>0.4849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lin3</td>\n",
       "      <td>0.01 vs 5.0</td>\n",
       "      <td>0.677672</td>\n",
       "      <td>0.648368</td>\n",
       "      <td>1.16531</td>\n",
       "      <td>0.246693</td>\n",
       "      <td>-0.029304</td>\n",
       "      <td>0.5495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lin3</td>\n",
       "      <td>0.01 vs 20.0</td>\n",
       "      <td>0.677672</td>\n",
       "      <td>0.704569</td>\n",
       "      <td>-0.60361</td>\n",
       "      <td>0.547484</td>\n",
       "      <td>0.026898</td>\n",
       "      <td>0.4852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lin3</td>\n",
       "      <td>0.01 vs 50.0</td>\n",
       "      <td>0.677672</td>\n",
       "      <td>0.720817</td>\n",
       "      <td>-0.716039</td>\n",
       "      <td>0.475653</td>\n",
       "      <td>0.043145</td>\n",
       "      <td>0.4951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lin4</td>\n",
       "      <td>0.01 vs 5.0</td>\n",
       "      <td>1.613871</td>\n",
       "      <td>1.614901</td>\n",
       "      <td>-0.129947</td>\n",
       "      <td>0.896872</td>\n",
       "      <td>0.00103</td>\n",
       "      <td>0.5019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lin4</td>\n",
       "      <td>0.01 vs 20.0</td>\n",
       "      <td>1.613871</td>\n",
       "      <td>1.581815</td>\n",
       "      <td>1.80267</td>\n",
       "      <td>0.074483</td>\n",
       "      <td>-0.032056</td>\n",
       "      <td>0.527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lin4</td>\n",
       "      <td>0.01 vs 50.0</td>\n",
       "      <td>1.613871</td>\n",
       "      <td>1.564916</td>\n",
       "      <td>2.325207</td>\n",
       "      <td>0.022102</td>\n",
       "      <td>-0.048955</td>\n",
       "      <td>0.5145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Reward Type    Alpha Pair Avg DR Rewards (No Exploration)  \\\n",
       "0         lin1   0.01 vs 5.0                        1.009613   \n",
       "1         lin1  0.01 vs 20.0                        1.009613   \n",
       "2         lin1  0.01 vs 50.0                        1.009613   \n",
       "3         lin2   0.01 vs 5.0                        2.062473   \n",
       "4         lin2  0.01 vs 20.0                        2.062473   \n",
       "5         lin2  0.01 vs 50.0                        2.062473   \n",
       "6         lin3   0.01 vs 5.0                        0.677672   \n",
       "7         lin3  0.01 vs 20.0                        0.677672   \n",
       "8         lin3  0.01 vs 50.0                        0.677672   \n",
       "9         lin4   0.01 vs 5.0                        1.613871   \n",
       "10        lin4  0.01 vs 20.0                        1.613871   \n",
       "11        lin4  0.01 vs 50.0                        1.613871   \n",
       "\n",
       "   Avg DR Rewards (With Exploration) Paired t-stat Paired p-value  \\\n",
       "0                           0.998694      0.874752       0.383826   \n",
       "1                           0.982926      1.047003       0.297648   \n",
       "2                           0.976413      1.088175        0.27916   \n",
       "3                           2.065524     -0.290891       0.771744   \n",
       "4                           2.085374     -1.981935       0.050259   \n",
       "5                           2.103124     -2.908459       0.004484   \n",
       "6                           0.648368       1.16531       0.246693   \n",
       "7                           0.704569      -0.60361       0.547484   \n",
       "8                           0.720817     -0.716039       0.475653   \n",
       "9                           1.614901     -0.129947       0.896872   \n",
       "10                          1.581815       1.80267       0.074483   \n",
       "11                          1.564916      2.325207       0.022102   \n",
       "\n",
       "   Observed Mean Diff Bootstrap p-value  \n",
       "0           -0.010919            0.5477  \n",
       "1           -0.026687             0.514  \n",
       "2             -0.0332            0.5081  \n",
       "3            0.003052            0.5176  \n",
       "4            0.022901            0.4967  \n",
       "5            0.040652            0.4849  \n",
       "6           -0.029304            0.5495  \n",
       "7            0.026898            0.4852  \n",
       "8            0.043145            0.4951  \n",
       "9             0.00103            0.5019  \n",
       "10          -0.032056             0.527  \n",
       "11          -0.048955            0.5145  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "def run_backtest_with_alpha_test_with_exploration(X, total_samples, n_actions, optimal_actions, context_dim, reward_type='linear', alpha_values=[0.1, 0.5, 1.0], epochs=20):\n",
    "    X_train, X_val, historical_actions, historical_rewards, optimal_actions_val, rewards_val = generate_data_and_split(\n",
    "        X, total_samples, n_actions, optimal_actions, reward_type)\n",
    "\n",
    "    # Convert PyTorch tensors to NumPy arrays if needed\n",
    "    if isinstance(X_train, torch.Tensor):\n",
    "        X_train = X_train.detach().numpy()\n",
    "    if isinstance(X_val, torch.Tensor):\n",
    "        X_val = X_val.detach().numpy()\n",
    "    if isinstance(historical_rewards, torch.Tensor):\n",
    "        historical_rewards = historical_rewards.detach().numpy()\n",
    "    if isinstance(historical_actions, torch.Tensor):\n",
    "        historical_actions = historical_actions.detach().numpy()\n",
    "\n",
    "    # Generate true rewards based on optimal actions\n",
    "    all_true_rewards = np.zeros((X_val.shape[0], n_actions))\n",
    "    reward_gen = RewardGenerator()\n",
    "    \n",
    "    for action in range(n_actions):\n",
    "        for i, opt_action in enumerate(optimal_actions_val):\n",
    "            if reward_type == 'lin1':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards1(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin2':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards2(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin3':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards3(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin4':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards4(X_val[i], action, opt_action)\n",
    "\n",
    "    # Create an empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['Reward Type', 'Alpha Pair', 'Avg DR Rewards (No Exploration)', 'Avg DR Rewards (With Exploration)', 'Paired t-stat', 'Paired p-value', 'Observed Mean Diff', 'Bootstrap p-value'])\n",
    "\n",
    "    # Store DR rewards for each alpha to compare later\n",
    "    dr_rewards_dict = {}\n",
    "\n",
    "    for alpha in alpha_values:\n",
    "        # Initialize LinUCB model with current alpha\n",
    "        lin_UCB_DR_model = LinUCB_DR(n_actions, context_dim, alpha=alpha)\n",
    "        lin_UCB_DR_model.calculate_propensity_scores(historical_actions)  # Calculate propensities internally\n",
    "        \n",
    "        # Train LinUCB model on historical data\n",
    "        for i in range(len(historical_actions)):\n",
    "            lin_UCB_DR_model.update(historical_actions[i], historical_rewards[i, historical_actions[i]], X_train[i])\n",
    "        \n",
    "        # Predict with LinUCB on validation set\n",
    "        lin_UCB_preds = lin_UCB_DR_model.predict(X_val)\n",
    "        lin_UCB_preds = np.argmax(lin_UCB_preds, axis=1)\n",
    "        \n",
    "        # Calculate doubly robust rewards\n",
    "        dr_rewards = lin_UCB_DR_model.doubly_robust_estimator(X_val, lin_UCB_preds, historical_rewards, historical_actions, all_true_rewards)\n",
    "        dr_rewards_avg = np.mean(dr_rewards)\n",
    "        \n",
    "        # Store the DR rewards for the current alpha value\n",
    "        dr_rewards_dict[alpha] = dr_rewards\n",
    "\n",
    "    # Perform paired t-test and bootstrap test between alpha=0.01 (without exploration) and other alphas\n",
    "    alpha_no_exploration = 0.01\n",
    "    dr_rewards_no_exploration = dr_rewards_dict[alpha_no_exploration]\n",
    "\n",
    "    for alpha_with_exploration in alpha_values:\n",
    "        if alpha_with_exploration == alpha_no_exploration:\n",
    "            continue  # Skip comparison of 0.01 with itself\n",
    "\n",
    "        dr_rewards_with_exploration = dr_rewards_dict[alpha_with_exploration]\n",
    "\n",
    "        # Paired t-test between DR rewards with and without exploration\n",
    "        t_stat, p_value_ttest = stats.ttest_rel(dr_rewards_no_exploration, dr_rewards_with_exploration)\n",
    "\n",
    "        # Bootstrap Test\n",
    "        n_bootstrap = 10000\n",
    "        differences = dr_rewards_with_exploration - dr_rewards_no_exploration\n",
    "        observed_mean_diff = np.mean(differences)\n",
    "        \n",
    "        # Bootstrap sampling\n",
    "        bootstrap_means = np.array([\n",
    "            np.mean(np.random.choice(differences, size=len(differences), replace=True)) \n",
    "            for _ in range(n_bootstrap)\n",
    "        ])\n",
    "        \n",
    "        # Calculate the p-value as the proportion of bootstrap samples with a mean greater than or equal to the observed mean difference\n",
    "        p_value_bootstrap = np.mean(bootstrap_means >= observed_mean_diff)\n",
    "        \n",
    "        # Append comparison results to the DataFrame\n",
    "        results_df = results_df.append({\n",
    "            'Reward Type': reward_type,\n",
    "            'Alpha Pair': f\"{alpha_no_exploration} vs {alpha_with_exploration}\",\n",
    "            'Avg DR Rewards (No Exploration)': np.mean(dr_rewards_no_exploration),\n",
    "            'Avg DR Rewards (With Exploration)': np.mean(dr_rewards_with_exploration),\n",
    "            'Paired t-stat': t_stat,\n",
    "            'Paired p-value': p_value_ttest,\n",
    "            'Observed Mean Diff': observed_mean_diff,\n",
    "            'Bootstrap p-value': p_value_bootstrap\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Running the function with a loop over different reward types and alpha values\n",
    "alpha_values = [0.01, 5.0, 20.0, 50.0]\n",
    "results_all = pd.DataFrame()\n",
    "for reward_type in ['lin1', 'lin2', 'lin3', 'lin4']:\n",
    "    result_df = run_backtest_with_alpha_test_with_exploration(X, total_samples=1000, n_actions=2, optimal_actions=optimal_actions, context_dim=X.shape[1], reward_type=reward_type, alpha_values=alpha_values)\n",
    "    results_all = pd.concat([results_all, result_df], ignore_index=True)\n",
    "\n",
    "# Display the final results DataFrame\n",
    "display(results_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b3929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
