{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d02e31f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doubly Robust Estimator - Variance: 0.051 Bias: 1.887 MSE: 3.612\n",
      "Inverse Propensity Score Estimator - Variance: 0.077 Bias: 2.301 MSE: 5.372\n",
      "Ordinary Estimator - Variance: 0.001 Bias: 4.366 MSE: 19.061\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Simulate a Dataset with Known Ground Truth\n",
    "np.random.seed(1)\n",
    "n_samples =500\n",
    "n_actions = 3\n",
    "n_features = 5\n",
    "\n",
    "# Context features\n",
    "X = np.random.normal(size=(n_samples, n_features))\n",
    "\n",
    "def true_reward(x, action, weights, interaction_weights=None, noise_std=1.0):\n",
    "    base_reward = 5 * (action == 0) + 3 * (action == 1) + 2 * (action == 2)\n",
    "    linear_reward = np.dot(x, weights)\n",
    "    interaction_term = (\n",
    "        sum(interaction_weights[i, j] * x[i] * x[j] for i in range(len(x)) for j in range(i + 1, len(x)))\n",
    "        if interaction_weights is not None else 0\n",
    "    )\n",
    "    # Add Gaussian noise to the reward\n",
    "    noise = np.random.normal(0, noise_std)\n",
    "    \n",
    "    return base_reward + linear_reward + interaction_term + noise\n",
    "\n",
    "\n",
    "\n",
    "# Generate random weights for the reward function\n",
    "random_weights = np.random.normal(size=n_features)\n",
    "\n",
    "# Generate rewards for each action (same as before)\n",
    "rewards = np.array([\n",
    "    true_reward(X[i], action, random_weights)\n",
    "    for i in range(n_samples) for action in range(n_actions)\n",
    "]).reshape(n_samples, n_actions)\n",
    "\n",
    "# Action probabilities and observed actions\n",
    "action_probs = np.array([0.05, 0.2, 0.75])\n",
    "actions = np.random.choice(n_actions, size=n_samples, p=action_probs)\n",
    "\n",
    "# Observed rewards based on actions\n",
    "observed_rewards = rewards[np.arange(n_samples), actions]\n",
    "\n",
    "# Propensity score for each chosen action under the historical policy\n",
    "propensities = action_probs[actions]\n",
    "\n",
    "# Define LinUCB Class with Action Information in Feature Vector\n",
    "class LinUCB:\n",
    "    def __init__(self, n_features, n_actions, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.n_features = n_features\n",
    "        self.n_actions = n_actions\n",
    "        self.A = np.identity(n_features + n_actions)  # Updated for extra action feature\n",
    "        self.b = np.zeros(n_features + n_actions)\n",
    "\n",
    "    def update(self, x, action, reward, lambda_reg=0.1):\n",
    "        # Add action as one-hot encoding to the feature vector\n",
    "        x_extended = np.concatenate([x, np.eye(self.n_actions)[action]])  # One-hot encode the action\n",
    "        self.A += np.outer(x_extended, x_extended) + lambda_reg * np.identity(len(x_extended))  # Add regularization\n",
    "        self.b += reward * x_extended\n",
    "\n",
    "    def predict(self, x, action):\n",
    "        # Ensure `x` is at least 2D\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "\n",
    "        # Create one-hot encoding for the action and reshape it to match x's shape along axis=0\n",
    "        one_hot_actions = np.eye(self.n_actions)[action].reshape(1, -1)\n",
    "\n",
    "        # If `x` has more than one sample, tile `one_hot_actions` to match the batch size\n",
    "        if x.shape[0] > 1:\n",
    "            one_hot_actions = np.tile(one_hot_actions, (x.shape[0], 1))\n",
    "\n",
    "        # Concatenate along the last axis to combine x and action encoding\n",
    "        x_extended = np.concatenate([x, one_hot_actions], axis=1)\n",
    "\n",
    "        inv_A = np.linalg.inv(self.A)\n",
    "        theta = inv_A @ self.b\n",
    "\n",
    "        # Return the prediction\n",
    "        return x_extended @ theta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the LinUCB model (single model for all actions)\n",
    "model = LinUCB(n_features=n_features, n_actions=n_actions)\n",
    "\n",
    "# Train the LinUCB model for each action\n",
    "for i in range(n_samples):\n",
    "    model.update(X[i], actions[i], observed_rewards[i])\n",
    "    \n",
    "\n",
    "\n",
    "def calculate_target_action_probs(model, X, num_actions):\n",
    "    \n",
    "    # Compute scores for each action\n",
    "    scores = np.array([model.predict(X, action) for action in range(num_actions)]).T\n",
    "    \n",
    "    # Apply softmax across the action dimension to get probability distribution\n",
    "    target_action_probs = softmax(scores, axis=1)\n",
    "    return target_action_probs\n",
    "\n",
    "# Example usage with `actions` and `X` data:\n",
    "num_actions = len(np.unique(actions))  # Determine the number of actions\n",
    "target_action_probs = calculate_target_action_probs(model, X, num_actions)\n",
    "\n",
    "\n",
    "\n",
    "def dr_estimator(X, actions, observed_rewards, propensities, models, target_action_probs):\n",
    "    dr_estimate = np.mean([\n",
    "        (reward - models[action].predict(X[i])) * target_action_probs[action] / propensities[i] + models[action].predict(X[i])\n",
    "        for i, (reward, action) in enumerate(zip(observed_rewards, actions))\n",
    "    ])\n",
    "    return dr_estimate\n",
    "\n",
    "\n",
    "def ips_estimator(X, actions, observed_rewards, propensities, target_action_probs):\n",
    "    # Select the target probability for each action taken\n",
    "    selected_target_probs = target_action_probs[np.arange(len(actions)), actions]\n",
    "    \n",
    "    # Calculate the IPS estimate using only the selected action's probability\n",
    "    ips_estimate = np.mean(observed_rewards * selected_target_probs / propensities)\n",
    "    return ips_estimate\n",
    "\n",
    "\n",
    "# Now we can use a single model for all actions\n",
    "def ordinary_estimator(X, actions, observed_rewards, propensities, model, target_action_probs):\n",
    "    model_rewards = np.mean([\n",
    "        target_action_probs[i] * model.predict(X[i], actions[i])  # Use the single model for all actions\n",
    "        for i in range(len(actions))\n",
    "    ])\n",
    "    return model_rewards\n",
    "\n",
    "# Calculate true policy value\n",
    "def calculate_true_policy_value(rewards):\n",
    "    optimal_rewards = [\n",
    "        max(true_reward(X[i], action, random_weights) for action in range(n_actions))\n",
    "        for i in range(n_samples)\n",
    "    ]\n",
    "    true_policy_value = np.mean(optimal_rewards)\n",
    "    return true_policy_value\n",
    "\n",
    "def dr_estimator(X, actions, observed_rewards, propensities, model, target_action_probs):\n",
    "    dr_estimate = np.mean([\n",
    "        (reward - model.predict(X[i], action)) * target_action_probs[i] / propensities[i] + model.predict(X[i], action)\n",
    "        for i, (reward, action) in enumerate(zip(observed_rewards, actions))\n",
    "    ])\n",
    "    return dr_estimate\n",
    "\n",
    "def bootstrap_variance(estimator_fn, X, actions, observed_rewards, propensities, true_policy_value, models=None, target_action_probs=None, n_bootstraps=50):\n",
    "    estimates = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        sample_indices = np.random.choice(len(X), len(X), replace=True)\n",
    "        X_sample = X[sample_indices]\n",
    "        actions_sample = actions[sample_indices]\n",
    "        rewards_sample = observed_rewards[sample_indices]\n",
    "        propensities_sample = propensities[sample_indices]\n",
    "\n",
    "        # Calculate estimate based on the estimator type\n",
    "        if estimator_fn in [dr_estimator, ordinary_estimator]:\n",
    "            estimate = estimator_fn(X_sample, actions_sample, rewards_sample, propensities_sample, models, target_action_probs)\n",
    "        else:\n",
    "            estimate = estimator_fn(X_sample, actions_sample, rewards_sample, propensities_sample, target_action_probs)\n",
    "\n",
    "        estimates.append(estimate)\n",
    "\n",
    "    estimates = np.array(estimates)\n",
    "    variance = np.var(estimates)\n",
    "    bias = abs(np.mean(estimates) - true_policy_value)\n",
    "    mse = bias ** 2 + variance\n",
    "    return variance, bias, mse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run bootstrapping for each estimator\n",
    "dr_variance, dr_bias, dr_mse = bootstrap_variance(dr_estimator, X, actions, observed_rewards, propensities, calculate_true_policy_value(observed_rewards), model, target_action_probs)\n",
    "ips_variance, ips_bias, ips_mse = bootstrap_variance(ips_estimator, X, actions, observed_rewards, propensities, calculate_true_policy_value(observed_rewards), target_action_probs=target_action_probs)\n",
    "simple_variance, simple_bias, simple_mse = bootstrap_variance(ordinary_estimator, X, actions, observed_rewards, propensities, calculate_true_policy_value(observed_rewards), model, target_action_probs)\n",
    "\n",
    "# Print results\n",
    "print(\"Doubly Robust Estimator - Variance:\", np.round(dr_variance,3), \"Bias:\", np.round(dr_bias,3), \"MSE:\", np.round(dr_mse,3))\n",
    "print(\"Inverse Propensity Score Estimator - Variance:\", np.round(ips_variance,3), \"Bias:\", np.round(ips_bias,3), \"MSE:\", np.round(ips_mse,3))\n",
    "print(\"Ordinary Estimator - Variance:\", np.round(simple_variance,3), \"Bias:\", np.round(simple_bias,3), \"MSE:\", np.round(simple_mse,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a23c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
