{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "284e071c-5fd1-452f-a55d-2d9640dc13e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# import tensorflow_probability as tfp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgpytorch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultivariateNormal\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gpytorch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import scipy.stats as stats\n",
    "# import tensorflow_probability as tfp\n",
    "import gpytorch\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import gamma, norm, beta\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder,  MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dece18-87b6-4582-98b9-96dd29463c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e4a88-1603-47b4-ae66-ef8f7b5cacc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ucimlrepo\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5cd352-757b-4370-8ccb-08daf15869dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5204855-0b0d-43d2-a35a-199b1bc4eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "statlog_german_credit_data = fetch_ucirepo(id=144) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "data = statlog_german_credit_data.data.features \n",
    "y = statlog_german_credit_data.data.targets.values - 1 ## 1 subtracted for (0 = Good,  1 = Bad) labelling\n",
    "  \n",
    "# metadata \n",
    "#print(statlog_german_credit_data.metadata) \n",
    "  \n",
    "# variable information \n",
    "#print(statlog_german_credit_data.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea97dd-5324-4a25-85dc-728348ab5669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data[statlog_german_credit_data.variables.name[:-1]]\n",
    "df_full=df.copy()\n",
    "df_full.columns=statlog_german_credit_data.variables.description[:-1].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef8bfde-a668-41ab-b146-048a693a8c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rellaxiao/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute2</th>\n",
       "      <th>Attribute5</th>\n",
       "      <th>Attribute8</th>\n",
       "      <th>Attribute11</th>\n",
       "      <th>Attribute13</th>\n",
       "      <th>Attribute16</th>\n",
       "      <th>Attribute18</th>\n",
       "      <th>x0_A11</th>\n",
       "      <th>x0_A12</th>\n",
       "      <th>x0_A13</th>\n",
       "      <th>...</th>\n",
       "      <th>x8_A143</th>\n",
       "      <th>x9_A151</th>\n",
       "      <th>x9_A152</th>\n",
       "      <th>x9_A153</th>\n",
       "      <th>x10_A171</th>\n",
       "      <th>x10_A172</th>\n",
       "      <th>x10_A173</th>\n",
       "      <th>x10_A174</th>\n",
       "      <th>Attribute19</th>\n",
       "      <th>Attribute20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.941176</td>\n",
       "      <td>-0.898867</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>-0.372620</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.892857</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-0.796853</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>-0.160119</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.411765</td>\n",
       "      <td>-0.491581</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-0.836470</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.571429</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-0.235294</td>\n",
       "      <td>-0.603059</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-0.939034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.321429</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>-0.824475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>-0.523935</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Attribute2  Attribute5  Attribute8  Attribute11  Attribute13  \\\n",
       "0     -0.941176   -0.898867    1.000000     1.000000     0.714286   \n",
       "1      0.294118   -0.372620   -0.333333    -0.333333    -0.892857   \n",
       "2     -0.764706   -0.796853   -0.333333     0.333333     0.071429   \n",
       "3      0.117647   -0.160119   -0.333333     1.000000    -0.071429   \n",
       "4     -0.411765   -0.491581    0.333333     1.000000     0.214286   \n",
       "..          ...         ...         ...          ...          ...   \n",
       "995   -0.764706   -0.836470    0.333333     1.000000    -0.571429   \n",
       "996   -0.235294   -0.603059    1.000000     1.000000    -0.250000   \n",
       "997   -0.764706   -0.939034    1.000000     1.000000    -0.321429   \n",
       "998    0.205882   -0.824475    1.000000     1.000000    -0.857143   \n",
       "999    0.205882   -0.523935    0.333333     1.000000    -0.714286   \n",
       "\n",
       "     Attribute16  Attribute18  x0_A11  x0_A12  x0_A13  ...  x8_A143  x9_A151  \\\n",
       "0      -0.333333         -1.0     1.0    -1.0    -1.0  ...      1.0     -1.0   \n",
       "1      -1.000000         -1.0    -1.0     1.0    -1.0  ...      1.0     -1.0   \n",
       "2      -1.000000          1.0    -1.0    -1.0    -1.0  ...      1.0     -1.0   \n",
       "3      -1.000000          1.0     1.0    -1.0    -1.0  ...      1.0     -1.0   \n",
       "4      -0.333333          1.0     1.0    -1.0    -1.0  ...      1.0     -1.0   \n",
       "..           ...          ...     ...     ...     ...  ...      ...      ...   \n",
       "995    -1.000000         -1.0    -1.0    -1.0    -1.0  ...      1.0     -1.0   \n",
       "996    -1.000000         -1.0     1.0    -1.0    -1.0  ...      1.0     -1.0   \n",
       "997    -1.000000         -1.0    -1.0    -1.0    -1.0  ...      1.0     -1.0   \n",
       "998    -1.000000         -1.0     1.0    -1.0    -1.0  ...      1.0     -1.0   \n",
       "999    -1.000000         -1.0    -1.0     1.0    -1.0  ...      1.0     -1.0   \n",
       "\n",
       "     x9_A152  x9_A153  x10_A171  x10_A172  x10_A173  x10_A174  Attribute19  \\\n",
       "0        1.0     -1.0      -1.0      -1.0       1.0      -1.0          1.0   \n",
       "1        1.0     -1.0      -1.0      -1.0       1.0      -1.0         -1.0   \n",
       "2        1.0     -1.0      -1.0       1.0      -1.0      -1.0         -1.0   \n",
       "3       -1.0      1.0      -1.0      -1.0       1.0      -1.0         -1.0   \n",
       "4       -1.0      1.0      -1.0      -1.0       1.0      -1.0         -1.0   \n",
       "..       ...      ...       ...       ...       ...       ...          ...   \n",
       "995      1.0     -1.0      -1.0       1.0      -1.0      -1.0         -1.0   \n",
       "996      1.0     -1.0      -1.0      -1.0      -1.0       1.0          1.0   \n",
       "997      1.0     -1.0      -1.0      -1.0       1.0      -1.0         -1.0   \n",
       "998     -1.0      1.0      -1.0      -1.0       1.0      -1.0          1.0   \n",
       "999      1.0     -1.0      -1.0      -1.0       1.0      -1.0         -1.0   \n",
       "\n",
       "     Attribute20  \n",
       "0           -1.0  \n",
       "1           -1.0  \n",
       "2           -1.0  \n",
       "3           -1.0  \n",
       "4           -1.0  \n",
       "..           ...  \n",
       "995         -1.0  \n",
       "996         -1.0  \n",
       "997         -1.0  \n",
       "998         -1.0  \n",
       "999         -1.0  \n",
       "\n",
       "[1000 rows x 59 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define preprocessing steps\n",
    "numeric_features = ['Attribute2', 'Attribute5', 'Attribute8', 'Attribute11', 'Attribute13', 'Attribute16', 'Attribute18']\n",
    "binary_features = ['Attribute19', 'Attribute20']\n",
    "categorical_features = ['Attribute1', 'Attribute3', 'Attribute4', 'Attribute6', 'Attribute7', 'Attribute9', 'Attribute10', 'Attribute12', 'Attribute14', 'Attribute15', 'Attribute17']\n",
    "\n",
    "# Apply LabelEncoder to binary features\n",
    "label_encoders = {}\n",
    "for feature in binary_features:\n",
    "    le = LabelEncoder()\n",
    "    df[feature] = le.fit_transform(df[feature])\n",
    "    label_encoders[feature] = le  # Store the encoder for future use (e.g., inverse transform)\n",
    "\n",
    "# Pipeline for numeric features: Imputation and Min-Max Scaling between -1 and 1\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler(feature_range=(-1, 1)))\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features: Imputation, OneHotEncoding, and Min-Max Scaling between -1 and 1\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False)),  # Set sparse=False for easy concatenation\n",
    "    ('scaler', MinMaxScaler(feature_range=(-1, 1)))  # Scaling the one-hot encoded features\n",
    "])\n",
    "\n",
    "# For binary features, use Min-Max Scaling as well\n",
    "binary_transformer = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler(feature_range=(-1, 1)))\n",
    "])\n",
    "\n",
    "# Combine the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('bin', binary_transformer, binary_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the preprocessing steps to the DataFrame\n",
    "df2 = preprocessor.fit_transform(df)\n",
    "\n",
    "# If you want to convert it back to a DataFrame for ease of use\n",
    "# Create column names for the one-hot encoded features\n",
    "onehot_feature_names = list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out())\n",
    "\n",
    "# Combine all feature names\n",
    "all_feature_names = numeric_features + onehot_feature_names + binary_features\n",
    "\n",
    "# Create the processed DataFrame\n",
    "df2 = pd.DataFrame(df2, columns=all_feature_names)\n",
    "\n",
    "# Show the processed DataFrame\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab017bc7-5d1a-47ab-a9fa-3043144988ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Define reward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb8850-86f9-4071-9fac-21ab650a6927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Call the seed setting function\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b13ba5c-4cbf-4aa7-8709-38182501a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RewardGenerator and generate_data functions (from your code)\n",
    "class RewardGenerator:\n",
    "    def __init__(self, drift_rate=0.005, seasonal_period=400):\n",
    "        self.drift_rate = drift_rate\n",
    "        self.seasonal_period = seasonal_period\n",
    "        self.t = 0  # Initialize time step\n",
    "    \n",
    "    def generate_rewards1(self, context, action, optimal_action, n = 3):\n",
    "        '''\n",
    "        Generates rewards using a simple linear function with noise.\n",
    "\n",
    "        Parameters:\n",
    "        - context: np.array, the feature context\n",
    "        - action: int, the action taken\n",
    "        - optimal_action: int, the optimal action for this context\n",
    "\n",
    "        Returns:\n",
    "        - reward: float, the generated reward\n",
    "        '''\n",
    "\n",
    "        # Sample `n` context features\n",
    "        if len(context) >= n:\n",
    "            sampled_context = np.random.choice(context, size=n, replace=False)\n",
    "        else:\n",
    "            sampled_context = context  # Use all features if fewer than n\n",
    "        \n",
    "        # Linear reward based on context sum\n",
    "        context_factor = np.sum(sampled_context)\n",
    "        \n",
    "        # Add a base reward depending on whether the action is optimal\n",
    "        base_reward = 1.5 if action == optimal_action else 0.8\n",
    "\n",
    "        # Linear reward calculation with noise\n",
    "        reward = base_reward + 0.1 * context_factor + np.random.normal(0, 0.1)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def generate_rewards2(self, context, action, optimal_action, n=3):\n",
    "        '''\n",
    "        Generates rewards with added complexity from context sampling and non-stationarity.\n",
    "\n",
    "        Parameters:\n",
    "        - context: np.array, the feature context\n",
    "        - action: int, the action taken\n",
    "        - optimal_action: int, the optimal action for this context\n",
    "        - n: int, number of context features to sample for reward generation\n",
    "\n",
    "        Returns:\n",
    "        - reward: float, the generated reward\n",
    "        '''\n",
    "        # Sample `n` context features\n",
    "        if len(context) >= n:\n",
    "            sampled_context = np.random.choice(context, size=n, replace=False)\n",
    "        else:\n",
    "            sampled_context = context  # Use all features if fewer than n\n",
    "        \n",
    "        # Compute a context factor based on the sampled features\n",
    "        context_factor = np.sum(sampled_context)\n",
    "        \n",
    "        # Non-stationarity (drift and seasonality)\n",
    "        time_drift = self.drift_rate * self.t\n",
    "        seasonality = np.sin(2 * np.pi * self.t / self.seasonal_period)\n",
    "\n",
    "        # Calculate reward based on action and non-stationary factors\n",
    "        if action == optimal_action:\n",
    "            mean = 1 + 0.2 * context_factor + time_drift + seasonality\n",
    "            std = 0.1  # Small variance for optimal action\n",
    "        else:\n",
    "            mean = 0 + 0.5 * context_factor + time_drift + seasonality\n",
    "            std = 0.4  # Higher variance for non-optimal action\n",
    "        \n",
    "        # Sample reward from Gaussian distribution\n",
    "        reward = np.random.normal(mean, std)\n",
    "\n",
    "        # Increment time step for next call\n",
    "        self.t += 1\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "\n",
    "    def generate_rewards3(self, context, action, optimal_action, n=3):\n",
    "        '''\n",
    "        Generates rewards using a piecewise linear function with context-dependent intervals.\n",
    "        '''\n",
    "        # Sample `n` context features\n",
    "        if len(context) >= n:\n",
    "            sampled_context = np.random.choice(context, size=n, replace=False)\n",
    "        else:\n",
    "            sampled_context = context  # Use all features if fewer than n\n",
    "\n",
    "        # Compute a context factor based on the sampled features\n",
    "        context_factor = np.sum(sampled_context)\n",
    "        \n",
    "        # Piecewise modification based on context factor\n",
    "        if context_factor < 0.5:\n",
    "            context_factor = 1\n",
    "        elif 0.5 <= context_factor < 0.6:\n",
    "            context_factor = -10 * context_factor + 4\n",
    "        else:\n",
    "            context_factor = 0\n",
    "        \n",
    "        # Non-stationarity (drift and seasonality)\n",
    "        time_drift = self.drift_rate * self.t\n",
    "        seasonality = np.sin(2 * np.pi * self.t / self.seasonal_period)\n",
    "\n",
    "        # Calculate reward based on action and non-stationary factors\n",
    "        if action == optimal_action:\n",
    "            mean = 1 + 0.3 * context_factor + time_drift + seasonality\n",
    "            std = 0.1  # Small variance for optimal action\n",
    "        else:\n",
    "            mean = 0 + 0.6 * context_factor + time_drift + seasonality\n",
    "            std = 0.3  # Higher variance for non-optimal action\n",
    "        \n",
    "        # Sample reward from Gaussian distribution\n",
    "        reward = np.random.normal(mean, std)\n",
    "\n",
    "        # Increment time step for next call\n",
    "        self.t += 1\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def generate_rewards4(self, context, action, optimal_action, n=3):\n",
    "        '''\n",
    "        Generates rewards with a weighted linear combination of context features, drift, and seasonality.\n",
    "        '''\n",
    "        # Sample `n` context features\n",
    "        if len(context) >= n:\n",
    "            sampled_context = np.random.choice(context, size=n, replace=False)\n",
    "        else:\n",
    "            sampled_context = context  # Use all features if fewer than n\n",
    "        \n",
    "        # Assign weights to context features\n",
    "        context_weights = np.array([0.3, 0.5, 0.2])[:len(sampled_context)]\n",
    "        # Weighted sum of context features\n",
    "        context_factor = np.dot(sampled_context, context_weights)  \n",
    "\n",
    "        # Non-stationary components: drift and seasonality\n",
    "        time_drift = self.drift_rate * self.t  \n",
    "        seasonality = 1.5 * np.sin(2 * np.pi * self.t / (self.seasonal_period * 4))\n",
    "\n",
    "        # Calculate reward based on action and non-stationary factors\n",
    "        if action == optimal_action:\n",
    "            mean = 1.2 + 0.4 * context_factor + time_drift + seasonality\n",
    "            std = 0.1  # Small variance for optimal action\n",
    "        else:\n",
    "            mean = 0.5 + 0.7 * context_factor + time_drift + seasonality\n",
    "            std = 0.3  # Higher variance for non-optimal action\n",
    "\n",
    "        # Sample reward from Gaussian distribution\n",
    "        reward = np.random.normal(mean, std)\n",
    "\n",
    "        # Increment time step for next call\n",
    "        self.t += 1\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def generate_rewards5(self, context, action, optimal_action, n=3):\n",
    "        '''\n",
    "        Reward function that models heteroscedastic rewards where reward variance depends on the context.\n",
    "        '''\n",
    "        # Randomly generate theta_heteroscedastic and theta_var\n",
    "        theta_heteroscedastic = np.random.randn(n)  # Random vector for mean reward\n",
    "        theta_var = np.random.randn(n)              # Random vector for variance\n",
    "\n",
    "        # Sample `n` context features\n",
    "        if len(context) >= n:\n",
    "            sampled_context = np.random.choice(context, size=n, replace=False)\n",
    "        else:\n",
    "            sampled_context = context  # Use all features if fewer than n\n",
    "    \n",
    "        # Linear reward based on context sum\n",
    "        context_factor = np.sum(sampled_context)\n",
    "    \n",
    "        # Calculate the mean reward (dot product with theta_heteroscedastic)\n",
    "        base_reward = np.dot(sampled_context, theta_heteroscedastic)\n",
    "    \n",
    "        # Calculate the context-dependent variance (dot product with theta_var)\n",
    "        context_variance = np.abs(np.dot(sampled_context, theta_var)) \n",
    "    \n",
    "        # Add a base reward depending on whether the action is optimal\n",
    "        optimal_bonus = 1.5 if action == optimal_action else 0.8\n",
    "    \n",
    "        # Linear reward calculation with noise\n",
    "        reward = optimal_bonus + base_reward + 0.1 * context_factor + np.random.normal(0, context_variance)\n",
    "        # reward = optimal_bonus + base_reward + np.random.normal(0, context_variance)\n",
    "        \n",
    "        return reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c9df6a-9203-4048-938f-7c66d1088085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage in your data generation function\n",
    "def generate_data(context, total_samples, n_actions, optimal_actions, reward_type):\n",
    "    rewards = torch.zeros(total_samples, n_actions)\n",
    "    \n",
    "    reward_gen = RewardGenerator()\n",
    "\n",
    "    for i in range(total_samples):\n",
    "        for action in range(n_actions):\n",
    "            # Generate rewards based on the given logic\n",
    "            if reward_type == 'lin1':\n",
    "                rewards[i, action] = reward_gen.generate_rewards1(context[i], action,  optimal_actions[i])\n",
    "            elif reward_type == 'lin2':\n",
    "                rewards[i, action] = reward_gen.generate_rewards2(context[i], action,  optimal_actions[i])\n",
    "            elif reward_type == 'lin3':\n",
    "                rewards[i, action] = reward_gen.generate_rewards3(context[i], action,  optimal_actions[i])\n",
    "            elif reward_type == 'lin4':\n",
    "                rewards[i, action] = reward_gen.generate_rewards4(context[i], action,  optimal_actions[i])\n",
    "            elif reward_type == 'lin5':\n",
    "                rewards[i, action] = reward_gen.generate_rewards5(context[i], action,  optimal_actions[i])   \n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# Function to generate actions from a normal distribution and clip them to valid action range\n",
    "# def generate_actions(total_samples):\n",
    "#    return  np.random.choice([0, 1], size=total_samples)\n",
    "\n",
    "def generate_actions(total_samples, bias=0.6):\n",
    "    # Generate actions with the specified bias\n",
    "    actions = np.random.choice([0, 1], size=total_samples, p=[1 - bias, bias])\n",
    "    return actions\n",
    "\n",
    "# Define the data generation and splitting function\n",
    "def generate_data_and_split(X, total_samples, n_actions, optimal_actions, reward_type):\n",
    "    \n",
    "    rewards = generate_data(X, total_samples, n_actions, optimal_actions, reward_type)\n",
    "    split_index = int(total_samples * 0.9)\n",
    "    X_train = X[:split_index]\n",
    "    historical_actions = generate_actions(split_index)\n",
    "    historical_rewards = rewards[:split_index]\n",
    "    \n",
    "    X_val = X[split_index:]\n",
    "    optimal_actions_val = optimal_actions[split_index:]\n",
    "    rewards_val = rewards[split_index:]\n",
    "    \n",
    "    return X_train, X_val, historical_actions, historical_rewards, optimal_actions_val, rewards_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9263e20-ffb8-429b-a32f-2c390b09289a",
   "metadata": {},
   "source": [
    "## 3.1 LinUCB with DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0bc74-0456-45df-ba8a-bc4dc668d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinUCB_DR:\n",
    "    def __init__(self, n_actions, context_dim, alpha=0.5):\n",
    "        self.n_actions = n_actions\n",
    "        self.context_dim = context_dim\n",
    "        self.alpha = alpha\n",
    "        self.A = [np.eye(context_dim) for _ in range(n_actions)]  # Identity matrices for each action\n",
    "        self.b = [np.zeros(context_dim) for _ in range(n_actions)]  # Zero vectors for each action\n",
    "        self.propensities = None\n",
    "\n",
    "    def calculate_propensity_scores(self, historical_actions):\n",
    "        # Calculate propensity scores based on historical action frequencies\n",
    "        total_samples = len(historical_actions)\n",
    "        action_freq = np.bincount(historical_actions, minlength=self.n_actions) / total_samples\n",
    "        self.propensities = action_freq[historical_actions]\n",
    "        return self.propensities\n",
    "\n",
    "    def update(self, action, reward, context):\n",
    "        # Update A and b for the taken action\n",
    "        self.A[action] += np.outer(context, context)\n",
    "        self.b[action] += reward * context\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict the reward for each action in the context X\n",
    "        p = np.zeros((X.shape[0], self.n_actions))\n",
    "        for a in range(self.n_actions):\n",
    "            theta_a = np.linalg.solve(self.A[a], self.b[a])  # Solve A * theta = b\n",
    "            p[:, a] = X @ theta_a + self.alpha * np.sqrt(np.sum(X @ np.linalg.inv(self.A[a]) * X, axis=1))\n",
    "        return p\n",
    "\n",
    "    def doubly_robust_estimator(self, X, chosen_actions, historical_rewards, historical_actions, all_true_rewards):\n",
    "        # Calculate doubly robust reward estimates\n",
    "        N = len(chosen_actions)\n",
    "        dr_rewards = np.zeros(N)\n",
    "        propensities = self.propensities if self.propensities is not None else self.calculate_propensity_scores(historical_actions)\n",
    "\n",
    "        numerator_all = np.exp(self.predict(X))\n",
    "        for i in range(N):\n",
    "            Ai = chosen_actions[i]\n",
    "            Ci = X[i]\n",
    "            Ri = historical_rewards[i, historical_actions[i]]\n",
    "            # pi_ratio = propensities[i] / propensities[chosen_actions[i]]\n",
    "            # pi_ratio = propensities[chosen_actions[i]] / propensities[historical_actions[i]]\n",
    "            \n",
    "            # numerator = p/sum(p)  (softmax)\n",
    "            # numerator = numerator_all[i] / np.sum(numerator_all[i])\n",
    "            pi_ratio = numerator_all[i, Ai] / np.sum(numerator_all[i])\n",
    "            # print(pi_ratio)\n",
    "\n",
    "            predicted_reward = all_true_rewards[i, Ai]\n",
    "            \n",
    "            # dr_rewards[i] = Ri * pi_ratio + 1 / pi_ratio * predicted_reward\n",
    "            dr_rewards[i] = pi_ratio * (Ri - predicted_reward) + predicted_reward\n",
    "\n",
    "        return dr_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a806a5-e131-4ee2-8ae4-0cb98e848b14",
   "metadata": {},
   "source": [
    "## 3.2 GPThompson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca66a5-396d-4081-a03a-69d2469d63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GP model\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, lengthscale=1.0, outputscale=1.0):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=2.5, lengthscale=lengthscale)\n",
    "        )\n",
    "        self.covar_module.outputscale = outputscale\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "        \n",
    "\n",
    "# Define the GP Thompson Sampling class\n",
    "class GPThompsonSampling:\n",
    "    def __init__(self, n_actions, context_dim, num_samples=10, lengthscale=1.0, outputscale=1.0):\n",
    "        self.n_actions = n_actions\n",
    "        self.context_dim = context_dim\n",
    "        self.num_samples = num_samples\n",
    "        self.lengthscale = lengthscale\n",
    "        self.outputscale = outputscale\n",
    "        self.models = [None] * n_actions\n",
    "        self.likelihoods = [gpytorch.likelihoods.GaussianLikelihood() for _ in range(n_actions)]\n",
    "        # use dr\n",
    "        self.propensities = None\n",
    "    \n",
    "    def update(self, actions, rewards, contexts):\n",
    "        contexts = torch.tensor(contexts, dtype=torch.float32)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "\n",
    "        for action in range(self.n_actions):\n",
    "            indices = (actions == action)\n",
    "            if indices.sum().item() == 0:\n",
    "                continue\n",
    "\n",
    "            X_train_action = contexts[indices]\n",
    "            y_train_action = rewards[indices, action]\n",
    "\n",
    "            # Initialize and train the GP model for this action\n",
    "            self.models[action] = ExactGPModel(X_train_action, y_train_action, self.likelihoods[action], \n",
    "                                               lengthscale=self.lengthscale, outputscale=self.outputscale)\n",
    "            self.models[action].train()\n",
    "            optimizer = torch.optim.Adam(self.models[action].parameters(), lr=0.1)\n",
    "            mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihoods[action], self.models[action])\n",
    "\n",
    "            for _ in range(100):  # Number of training epochs\n",
    "                optimizer.zero_grad()\n",
    "                output = self.models[action](X_train_action)\n",
    "                \n",
    "                if y_train_action.dim() > 1:\n",
    "                    y_train_action = y_train_action.squeeze(-1)\n",
    "\n",
    "                loss = -mll(output, y_train_action)\n",
    "                \n",
    "                if loss.numel() != 1:\n",
    "                    raise ValueError(f\"Expected scalar loss, but got shape {loss.shape}\")\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "    def predict(self, context):\n",
    "        context = torch.tensor(context, dtype=torch.float32)\n",
    "        means = np.zeros((context.shape[0], self.n_actions))\n",
    "        stddevs = np.zeros((context.shape[0], self.n_actions))\n",
    "        for action in range(self.n_actions):\n",
    "            if self.models[action] is None:\n",
    "                means[:, action] = np.random.randn(context.shape[0])\n",
    "                stddevs[:, action] = np.ones(context.shape[0])\n",
    "            else:\n",
    "                self.models[action].eval()\n",
    "                gp_samples = self.sample_from_gp_posterior(self.models[action], context, self.num_samples)\n",
    "                means[:, action] = gp_samples.mean(dim=0).numpy()\n",
    "                stddevs[:, action] = gp_samples.std(dim=0).numpy()\n",
    "        return means, stddevs\n",
    "\n",
    "    def sample_from_gp_posterior(self, model, context, num_samples):\n",
    "        with torch.no_grad():\n",
    "            posterior = model(context)\n",
    "            samples = posterior.sample(sample_shape=torch.Size([num_samples]))\n",
    "        return samples\n",
    "\n",
    "    def calculate_propensity_scores(self, historical_actions):\n",
    "        # Calculate propensity scores based on historical action frequencies\n",
    "        total_samples = len(historical_actions)\n",
    "        action_freq = np.bincount(historical_actions, minlength=self.n_actions) / total_samples\n",
    "        self.propensities = action_freq[historical_actions]\n",
    "        return self.propensities\n",
    "    \n",
    "    def doubly_robust_estimator(self, X, chosen_actions, historical_rewards, historical_actions, all_true_rewards):\n",
    "        # Calculate doubly robust reward estimates\n",
    "        N = len(chosen_actions)\n",
    "        dr_rewards = np.zeros(N)\n",
    "        propensities = self.propensities if self.propensities is not None else self.calculate_propensity_scores(historical_actions)\n",
    "\n",
    "        numerator_all = np.exp(self.predict(X)[0])\n",
    "        for i in range(N):\n",
    "            Ai = chosen_actions[i]\n",
    "            Ci = X[i]\n",
    "            Ri = historical_rewards[i, historical_actions[i]]\n",
    "            pi_ratio = numerator_all[i, Ai] / np.sum(numerator_all[i])\n",
    "            predicted_reward = all_true_rewards[i, Ai]\n",
    "            \n",
    "            dr_rewards[i] = pi_ratio * (Ri - predicted_reward) + predicted_reward\n",
    "        return dr_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ae2b21-9292-4acf-8c30-887695ee7092",
   "metadata": {},
   "source": [
    "## 4. Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53381b-fe51-466d-bc4e-918927e648ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backtest_DR(X, total_samples, n_actions, optimal_actions, context_dim, num_samples=10, lengthscale=1.0, outputscale=1.0, epochs=20, reward_type='lin1'):\n",
    "    # Generate and split the dataset\n",
    "    X_train, X_val, historical_actions, historical_rewards, optimal_actions_val, rewards_val = generate_data_and_split(\n",
    "        X, total_samples, n_actions, optimal_actions, reward_type)\n",
    "\n",
    "    # Convert PyTorch tensors to NumPy arrays if needed\n",
    "    if isinstance(X_train, torch.Tensor):\n",
    "        X_train = X_train.detach().numpy()\n",
    "    if isinstance(X_val, torch.Tensor):\n",
    "        X_val = X_val.detach().numpy()\n",
    "    if isinstance(historical_rewards, torch.Tensor):\n",
    "        historical_rewards = historical_rewards.detach().numpy()\n",
    "    if isinstance(historical_actions, torch.Tensor):\n",
    "        historical_actions = historical_actions.detach().numpy()\n",
    "     \n",
    "    # GP Thompson Sampling\n",
    "    gp_ts_model = GPThompsonSampling(n_actions, context_dim, num_samples, lengthscale, outputscale)\n",
    "    gp_ts_model.update(historical_actions, historical_rewards, X_train)\n",
    "    gp_ts_means, gp_ts_stddevs = gp_ts_model.predict(X_val)\n",
    "    gp_ts_preds = np.argmax(gp_ts_means, axis=1)\n",
    "    \n",
    "    \n",
    "    # LinUCB_DR model\n",
    "    lin_UCB_DR_model = LinUCB_DR(n_actions, context_dim, alpha=0.5)\n",
    "    lin_UCB_DR_model.calculate_propensity_scores(historical_actions)  # Calculate propensities internally\n",
    "    \n",
    "    # Update the model with historical data\n",
    "    for i in range(len(historical_actions)):\n",
    "        lin_UCB_DR_model.update(historical_actions[i], historical_rewards[i, historical_actions[i]], X_train[i])\n",
    "    \n",
    "    # Generate predictions on validation data\n",
    "    lin_UCB_preds = lin_UCB_DR_model.predict(X_val)\n",
    "    lin_UCB_preds = np.argmax(lin_UCB_preds, axis=1)\n",
    "\n",
    "    \n",
    "    # Generate true rewards based on optimal actions\n",
    "    all_true_rewards = np.zeros((X_val.shape[0], n_actions))\n",
    "    \n",
    "    reward_gen = RewardGenerator()\n",
    "    \n",
    "    for action in range(n_actions):\n",
    "        for i, opt_action in enumerate(optimal_actions_val):\n",
    "            if reward_type == 'lin1':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards1(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin2':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards2(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin3':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards3(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin4':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards4(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin5':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards5(X_val[i], action, opt_action)\n",
    "\n",
    "    # true_rewards = np.max(all_true_rewards, axis=1)\n",
    "    \n",
    "    # Calculate doubly robust rewards\n",
    "    linucb_rewards = lin_UCB_DR_model.doubly_robust_estimator(X_val, lin_UCB_preds, historical_rewards, historical_actions, all_true_rewards)\n",
    "    gp_ts_rewards = gp_ts_model.doubly_robust_estimator(X_val, gp_ts_preds, historical_rewards, historical_actions, all_true_rewards)\n",
    "\n",
    "    # return average rewards\n",
    "    print(f\"Average Rewards LinUCB: {np.mean(linucb_rewards):.4f}, LinUCB Accuracy: {accuracy_score(optimal_actions_val, lin_UCB_preds):.4f}\")\n",
    "    #print(f\"LinUCB Accuracy: {accuracy_score(optimal_actions_val, lin_UCB_preds):.4f}\")\n",
    "    \n",
    "    print(f\"Average Rewards GPTS: {np.mean(gp_ts_rewards):.4f}, GPTS Accuracy: {accuracy_score(optimal_actions_val, gp_ts_preds):.4f}\")\n",
    "    #print(f\"GPTS Accuracy: {accuracy_score(optimal_actions_val, gp_ts_preds):.4f}\")\n",
    "    #print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a464f52f-ff3b-45fe-aa78-a46d5bbf86e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "X = df2.values\n",
    "total_samples = X.shape[0]\n",
    "n_actions = 2\n",
    "context_dim = X.shape[1]   # Number of context features\n",
    "optimal_actions = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e53220-2795-45b0-8823-bf2f7b31bafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rewards LinUCB: 1.0415, LinUCB Accuracy: 0.7500\n",
      "Average Rewards GPTS: 1.0448, GPTS Accuracy: 0.7700\n",
      "Average Rewards LinUCB: 1.1209, LinUCB Accuracy: 0.6300\n",
      "Average Rewards GPTS: 1.1314, GPTS Accuracy: 0.6300\n",
      "Average Rewards LinUCB: 2.0445, LinUCB Accuracy: 0.5200\n",
      "Average Rewards GPTS: 2.0916, GPTS Accuracy: 0.5500\n",
      "Average Rewards LinUCB: 1.5758, LinUCB Accuracy: 0.6400\n",
      "Average Rewards GPTS: 1.6234, GPTS Accuracy: 0.6000\n",
      "Average Rewards LinUCB: 1.0236, LinUCB Accuracy: 0.6300\n",
      "Average Rewards GPTS: 1.0389, GPTS Accuracy: 0.6100\n"
     ]
    }
   ],
   "source": [
    "run_backtest_DR(X, total_samples, n_actions, optimal_actions, context_dim,  num_samples=10, lengthscale=1.0, outputscale=1.0, epochs=50)\n",
    "run_backtest_DR(X, total_samples, n_actions, optimal_actions, context_dim,  num_samples=10, lengthscale=1.0, outputscale=1.0, epochs=50,reward_type='lin2')\n",
    "run_backtest_DR(X, total_samples, n_actions, optimal_actions, context_dim,  num_samples=10, lengthscale=1.0, outputscale=1.0, epochs=50,reward_type='lin3')\n",
    "run_backtest_DR(X, total_samples, n_actions, optimal_actions, context_dim,  num_samples=10, lengthscale=1.0, outputscale=1.0, epochs=50,reward_type='lin4')\n",
    "run_backtest_DR(X, total_samples, n_actions, optimal_actions, context_dim,  num_samples=10, lengthscale=1.0, outputscale=1.0, epochs=50,reward_type='lin5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3410f7a-4a5c-4f2f-8619-7f4de5327b5b",
   "metadata": {},
   "source": [
    "## 5. Different alpha for LinUCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0183950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backtest_with_alpha(X, total_samples, n_actions, optimal_actions, context_dim, reward_type='linear', alpha_values=[0.1, 0.5, 1.0], epochs=20):\n",
    "    X_train, X_val, historical_actions, historical_rewards, optimal_actions_val, rewards_val = generate_data_and_split(\n",
    "        X, total_samples, n_actions, optimal_actions, reward_type)\n",
    "\n",
    "    # Convert PyTorch tensors to NumPy arrays if needed\n",
    "    if isinstance(X_train, torch.Tensor):\n",
    "        X_train = X_train.detach().numpy()\n",
    "    if isinstance(X_val, torch.Tensor):\n",
    "        X_val = X_val.detach().numpy()\n",
    "    if isinstance(historical_rewards, torch.Tensor):\n",
    "        historical_rewards = historical_rewards.detach().numpy()\n",
    "    if isinstance(historical_actions, torch.Tensor):\n",
    "        historical_actions = historical_actions.detach().numpy()\n",
    "\n",
    "    # Generate true rewards based on optimal actions\n",
    "    all_true_rewards = np.zeros((X_val.shape[0], n_actions))\n",
    "    reward_gen = RewardGenerator()\n",
    "    \n",
    "    for action in range(n_actions):\n",
    "        for i, opt_action in enumerate(optimal_actions_val):\n",
    "            if reward_type == 'lin1':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards1(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin2':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards2(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin3':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards3(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin4':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards4(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin5':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards5(X_val[i], action, opt_action)\n",
    "\n",
    "    # Loop over alpha values and calculate DR rewards\n",
    "    for alpha in alpha_values:\n",
    "        \n",
    "        # Initialize LinUCB_DR model with the current alpha\n",
    "        lin_UCB_DR_model = LinUCB_DR(n_actions, context_dim, alpha=alpha)\n",
    "        lin_UCB_DR_model.calculate_propensity_scores(historical_actions)  # Calculate propensities internally\n",
    "        \n",
    "        # Update the model with historical data\n",
    "        for i in range(len(historical_actions)):\n",
    "            lin_UCB_DR_model.update(historical_actions[i], historical_rewards[i, historical_actions[i]], X_train[i])\n",
    "\n",
    "        # Generate predictions on validation data\n",
    "        lin_UCB_preds = lin_UCB_DR_model.predict(X_val)\n",
    "        lin_UCB_preds = np.argmax(lin_UCB_preds, axis=1)\n",
    "        \n",
    "        \n",
    "        # Calculate doubly robust rewards\n",
    "        dr_rewards = lin_UCB_DR_model.doubly_robust_estimator(X_val, lin_UCB_preds, historical_rewards, historical_actions, all_true_rewards)\n",
    "        \n",
    "        # Output average DR rewards for the current alpha\n",
    "        print(f\"Alpha: {alpha}, Average DR Rewards: {np.mean(dr_rewards):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1f0cd2-2b00-487b-8cbc-18f70d397977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Average DR Rewards: 1.0497\n",
      "Alpha: 0.1, Average DR Rewards: 1.0546\n",
      "Alpha: 1.0, Average DR Rewards: 1.0529\n",
      "Alpha: 2.0, Average DR Rewards: 1.0371\n",
      "Alpha: 5.0, Average DR Rewards: 1.0228\n",
      "Alpha: 10.0, Average DR Rewards: 1.0135\n",
      "Alpha: 15.0, Average DR Rewards: 1.0086\n",
      "Alpha: 20.0, Average DR Rewards: 1.0047\n",
      "Alpha: 50.0, Average DR Rewards: 0.9949\n"
     ]
    }
   ],
   "source": [
    "alpha_values = [0.01, 0.1, 1.0, 2.0, 5.0, 10.0, 15.0, 20.0, 50.0]\n",
    "run_backtest_with_alpha(X, total_samples=1000, n_actions=2, optimal_actions=optimal_actions, context_dim=X.shape[1], reward_type='lin1', alpha_values=alpha_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df51ae6f-4ed6-4b52-a9e0-b7c6e6542141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Average DR Rewards: 0.8731\n",
      "Alpha: 0.1, Average DR Rewards: 0.8735\n",
      "Alpha: 1.0, Average DR Rewards: 0.8894\n",
      "Alpha: 2.0, Average DR Rewards: 0.8816\n",
      "Alpha: 5.0, Average DR Rewards: 0.8670\n",
      "Alpha: 10.0, Average DR Rewards: 0.8638\n",
      "Alpha: 15.0, Average DR Rewards: 0.8504\n",
      "Alpha: 20.0, Average DR Rewards: 0.8183\n",
      "Alpha: 50.0, Average DR Rewards: 0.7546\n"
     ]
    }
   ],
   "source": [
    "run_backtest_with_alpha(X, total_samples=1000, n_actions=2, optimal_actions=optimal_actions, context_dim=X.shape[1], reward_type='lin2', alpha_values=alpha_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e423edb-b328-4f43-ae0b-b02418730f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Average DR Rewards: 2.0868\n",
      "Alpha: 0.1, Average DR Rewards: 2.0868\n",
      "Alpha: 1.0, Average DR Rewards: 2.0839\n",
      "Alpha: 2.0, Average DR Rewards: 2.0866\n",
      "Alpha: 5.0, Average DR Rewards: 2.0866\n",
      "Alpha: 10.0, Average DR Rewards: 2.0758\n",
      "Alpha: 15.0, Average DR Rewards: 2.0971\n",
      "Alpha: 20.0, Average DR Rewards: 2.0823\n",
      "Alpha: 50.0, Average DR Rewards: 2.0913\n"
     ]
    }
   ],
   "source": [
    "run_backtest_with_alpha(X, total_samples=1000, n_actions=2, optimal_actions=optimal_actions, context_dim=X.shape[1], reward_type='lin3', alpha_values=alpha_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f7a861-e3c5-411e-99b0-8cc5670caafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Average DR Rewards: 1.6129\n",
      "Alpha: 0.1, Average DR Rewards: 1.6135\n",
      "Alpha: 1.0, Average DR Rewards: 1.6079\n",
      "Alpha: 2.0, Average DR Rewards: 1.5888\n",
      "Alpha: 5.0, Average DR Rewards: 1.5885\n",
      "Alpha: 10.0, Average DR Rewards: 1.5638\n",
      "Alpha: 15.0, Average DR Rewards: 1.5766\n",
      "Alpha: 20.0, Average DR Rewards: 1.5737\n",
      "Alpha: 50.0, Average DR Rewards: 1.5982\n"
     ]
    }
   ],
   "source": [
    "run_backtest_with_alpha(X, total_samples=1000, n_actions=2, optimal_actions=optimal_actions, context_dim=X.shape[1], reward_type='lin4', alpha_values=alpha_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe8864-c7fd-492e-8fe0-0527b3c2d377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, Average DR Rewards: 1.0490\n",
      "Alpha: 0.1, Average DR Rewards: 1.0492\n",
      "Alpha: 1.0, Average DR Rewards: 1.0568\n",
      "Alpha: 2.0, Average DR Rewards: 1.0605\n",
      "Alpha: 5.0, Average DR Rewards: 1.0847\n",
      "Alpha: 10.0, Average DR Rewards: 1.0804\n",
      "Alpha: 15.0, Average DR Rewards: 1.0638\n",
      "Alpha: 20.0, Average DR Rewards: 1.0304\n",
      "Alpha: 50.0, Average DR Rewards: 0.9163\n"
     ]
    }
   ],
   "source": [
    "run_backtest_with_alpha(X, total_samples=1000, n_actions=2, optimal_actions=optimal_actions, context_dim=X.shape[1], reward_type='lin5', alpha_values=alpha_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc61737c",
   "metadata": {},
   "source": [
    "## 6. Statistical Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca97a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinUCB_DR_bias_variance:\n",
    "    def __init__(self, n_actions, context_dim, alpha=0.5):\n",
    "        self.n_actions = n_actions\n",
    "        self.context_dim = context_dim\n",
    "        self.alpha = alpha\n",
    "        self.A = [np.eye(context_dim) for _ in range(n_actions)]\n",
    "        self.b = [np.zeros(context_dim) for _ in range(n_actions)]\n",
    "        self.propensities = None\n",
    "\n",
    "    def update(self, action, reward, context):\n",
    "        \"\"\"Update A and b matrices for the selected action based on context and observed reward.\"\"\"\n",
    "        self.A[action] += np.outer(context, context)\n",
    "        self.b[action] += reward * context\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict expected rewards for each action given context X using LinUCB framework.\"\"\"\n",
    "        p = np.zeros((X.shape[0], self.n_actions))\n",
    "        for a in range(self.n_actions):\n",
    "            theta_a = np.linalg.solve(self.A[a], self.b[a])\n",
    "            p[:, a] = X @ theta_a + self.alpha * np.sqrt(np.sum(X @ np.linalg.inv(self.A[a]) * X, axis=1))\n",
    "        return p\n",
    "\n",
    "    def calculate_propensity_scores(self, historical_actions, imbalance_ratio=None):\n",
    "        \"\"\"Calculate propensity scores based on action frequencies, optionally applying imbalance.\"\"\"\n",
    "        total_samples = len(historical_actions)\n",
    "        action_freq = np.bincount(historical_actions, minlength=self.n_actions) / total_samples\n",
    "        if imbalance_ratio is not None:\n",
    "            action_freq = np.clip(action_freq * imbalance_ratio, 0, 1)\n",
    "        self.propensities = action_freq[historical_actions]\n",
    "        # Debug: print propensities\n",
    "        # print(\"Propensities:\", self.propensities[:10])  # Print first 10 for a quick check\n",
    "        return self.propensities\n",
    "\n",
    "    def doubly_robust_estimator(self, X, chosen_actions, historical_rewards, historical_actions, propensities, all_true_rewards):\n",
    "        \"\"\"Compute DR rewards and calculate bias and variance of the estimator.\"\"\"\n",
    "        N = len(chosen_actions)\n",
    "        dr_rewards = np.zeros(N)\n",
    "        propensities = propensities if propensities is not None else self.propensities\n",
    "\n",
    "        numerator_all = np.exp(self.predict(X))\n",
    "        for i in range(N):\n",
    "            Ai = chosen_actions[i]\n",
    "            Ri = historical_rewards[i, historical_actions[i]]\n",
    "            pi_ratio = numerator_all[i, Ai] / np.sum(numerator_all[i])\n",
    "            predicted_reward = all_true_rewards[i, Ai]\n",
    "            dr_rewards[i] = pi_ratio * (Ri - predicted_reward) + predicted_reward\n",
    "\n",
    "        return dr_rewards\n",
    "    \n",
    "def calculate_true_policy_value(all_true_rewards):\n",
    "    \"\"\"Compute the true policy value as the mean of optimal rewards.\"\"\"\n",
    "    optimal_rewards = np.max(all_true_rewards, axis=1)\n",
    "    return np.mean(optimal_rewards)\n",
    "\n",
    "def bootstrap_variance(estimator_fn, X, chosen_actions, historical_rewards, historical_actions, propensities, all_true_rewards, n_bootstraps=50):\n",
    "    \"\"\"Calculate variance, bias, and MSE using bootstrapping for the given estimator.\"\"\"\n",
    "    estimates = []\n",
    "    true_policy_value = calculate_true_policy_value(all_true_rewards)\n",
    "    \n",
    "    for _ in range(n_bootstraps):\n",
    "        # Resample the data with replacement\n",
    "        sample_indices = np.random.choice(len(X), len(X), replace=True)\n",
    "        X_sample = X[sample_indices]\n",
    "        chosen_actions_sample = chosen_actions[sample_indices]\n",
    "        historical_rewards_sample = historical_rewards[sample_indices]\n",
    "        historical_actions_sample = historical_actions[sample_indices]\n",
    "        propensities_sample = propensities[sample_indices]  # Ensure propensity scores align with resampled data\n",
    "\n",
    "        # Calculate the estimator's output for the resampled data\n",
    "        estimate = estimator_fn(X_sample, chosen_actions_sample, historical_rewards_sample, historical_actions_sample, propensities_sample, all_true_rewards)\n",
    "        estimates.append(np.mean(estimate))\n",
    "\n",
    "    estimates = np.array(estimates)\n",
    "    variance = np.var(estimates)\n",
    "    bias = abs(np.mean(estimates) - true_policy_value)\n",
    "    mse = bias ** 2 + variance\n",
    "    \n",
    "    # # Debug: Print final bias, variance, and mse for validation\n",
    "    # print(\"Bias:\", bias, \"Variance:\", variance, \"MSE:\", mse)\n",
    "    return variance, bias, mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f022fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backtest_with_alpha_test(X, total_samples, n_actions, optimal_actions, context_dim, reward_type='linear', alpha_values=[0.1, 0.5, 1.0], epochs=20, imbalance_ratio=None):\n",
    "    X_train, X_val, historical_actions, historical_rewards, optimal_actions_val, rewards_val = generate_data_and_split(\n",
    "        X, total_samples, n_actions, optimal_actions, reward_type)\n",
    "    # Convert PyTorch tensors to NumPy arrays if needed\n",
    "    if isinstance(X_train, torch.Tensor):\n",
    "        X_train = X_train.detach().numpy()\n",
    "    if isinstance(X_val, torch.Tensor):\n",
    "        X_val = X_val.detach().numpy()\n",
    "    if isinstance(historical_rewards, torch.Tensor):\n",
    "        historical_rewards = historical_rewards.detach().numpy()\n",
    "    if isinstance(historical_actions, torch.Tensor):\n",
    "        historical_actions = historical_actions.detach().numpy()\n",
    "\n",
    "    # Generate true rewards based on optimal actions\n",
    "    all_true_rewards = np.zeros((X_val.shape[0], n_actions))\n",
    "    reward_gen = RewardGenerator()\n",
    "    \n",
    "    for action in range(n_actions):\n",
    "        for i, opt_action in enumerate(optimal_actions_val):\n",
    "            if reward_type == 'lin1':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards1(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin2':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards2(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin3':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards3(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin4':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards4(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin5':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards5(X_val[i], action, opt_action)\n",
    "    \n",
    "    results_df = pd.DataFrame(columns=['Reward Type', 'Alpha', 'Avg DR Rewards', 'Avg LinUCB Rewards', \n",
    "                                       'Paired t-stat', 'Paired p-value', 'Observed Mean Diff', \n",
    "                                       'Bootstrap p-value', 'Bias DR', 'Bias Non-DR', 'Variance DR', 'Variance Non-DR'])\n",
    "    \n",
    "    for alpha in alpha_values:\n",
    "        lin_UCB_DR_model = LinUCB_DR_bias_variance(n_actions, context_dim, alpha=alpha)\n",
    "        lin_UCB_DR_model.calculate_propensity_scores(historical_actions, imbalance_ratio=imbalance_ratio)\n",
    "        \n",
    "        # Train the LinUCB model\n",
    "        for i in range(len(historical_actions)):\n",
    "            lin_UCB_DR_model.update(historical_actions[i], historical_rewards[i, historical_actions[i]], X_train[i])\n",
    "        \n",
    "        # Predict actions using LinUCB\n",
    "        lin_UCB_preds = np.argmax(lin_UCB_DR_model.predict(X_val), axis=1)\n",
    "\n",
    "        # Calculate DR rewards\n",
    "        dr_rewards = lin_UCB_DR_model.doubly_robust_estimator(X_val, lin_UCB_preds, historical_rewards, historical_actions, lin_UCB_DR_model.propensities, all_true_rewards)\n",
    "        dr_rewards_avg = np.mean(dr_rewards)\n",
    "        \n",
    "        # Calculate rewards for Non-DR estimator (LinUCB)\n",
    "        lin_UCB_rewards = np.array([all_true_rewards[i, lin_UCB_preds[i]] for i in range(len(lin_UCB_preds))])\n",
    "        lin_UCB_rewards_avg = np.mean(lin_UCB_rewards)\n",
    "        \n",
    "        # Perform bootstrapping to calculate bias, variance, and MSE for DR and Non-DR estimators\n",
    "        variance_dr, bias_dr, mse_dr = bootstrap_variance(\n",
    "            lin_UCB_DR_model.doubly_robust_estimator,\n",
    "            X_val, lin_UCB_preds, historical_rewards, historical_actions, \n",
    "            lin_UCB_DR_model.propensities, all_true_rewards\n",
    "        )\n",
    "\n",
    "        # For Non-DR estimator (ensure resampling is consistent)\n",
    "        variance_non_dr, bias_non_dr, mse_non_dr = bootstrap_variance(\n",
    "            lambda X, chosen_actions, hr, ha, ps, atr: np.array([atr[i, chosen_actions[i]] for i in range(len(chosen_actions))]),\n",
    "            X_val, lin_UCB_preds, historical_rewards, historical_actions,\n",
    "            lin_UCB_DR_model.propensities, all_true_rewards\n",
    "        )\n",
    "        \n",
    "        # Paired t-test between DR and Non-DR rewards\n",
    "        t_stat, p_value_ttest = stats.ttest_rel(dr_rewards, lin_UCB_rewards)\n",
    "\n",
    "        # Bootstrap Test\n",
    "        n_bootstrap = 10000\n",
    "        differences = dr_rewards - lin_UCB_rewards\n",
    "        observed_mean_diff = np.mean(differences)\n",
    "        \n",
    "        bootstrap_means = np.array([\n",
    "            np.mean(np.random.choice(differences, size=len(differences), replace=True)) \n",
    "            for _ in range(n_bootstrap)\n",
    "        ])\n",
    "        \n",
    "        p_value_bootstrap = np.mean(bootstrap_means >= observed_mean_diff)\n",
    "        \n",
    "        # Append results to the DataFrame\n",
    "        temp_df = pd.DataFrame([{\n",
    "            'Reward Type': reward_type,\n",
    "            'Alpha': alpha,\n",
    "            'Avg DR Rewards': dr_rewards_avg,\n",
    "            'Avg LinUCB Rewards': lin_UCB_rewards_avg,\n",
    "            'Paired t-stat': t_stat,\n",
    "            'Paired p-value': p_value_ttest,\n",
    "            'Observed Mean Diff': observed_mean_diff,\n",
    "            'Bootstrap p-value': p_value_bootstrap,\n",
    "            'Bias DR': bias_dr,\n",
    "            'Bias Non-DR': bias_non_dr,\n",
    "            'Variance DR': variance_dr,\n",
    "            'Variance Non-DR': variance_non_dr\n",
    "        }])\n",
    "\n",
    "        results_df = pd.concat([results_df, temp_df], ignore_index=True)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e0ec25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reward Type</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Avg DR Rewards</th>\n",
       "      <th>Avg LinUCB Rewards</th>\n",
       "      <th>Paired t-stat</th>\n",
       "      <th>Paired p-value</th>\n",
       "      <th>Observed Mean Diff</th>\n",
       "      <th>Bootstrap p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lin1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.036236</td>\n",
       "      <td>1.219565</td>\n",
       "      <td>-6.047755</td>\n",
       "      <td>2.615536e-08</td>\n",
       "      <td>-0.183329</td>\n",
       "      <td>0.4973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lin1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.036087</td>\n",
       "      <td>1.219565</td>\n",
       "      <td>-6.047007</td>\n",
       "      <td>2.624411e-08</td>\n",
       "      <td>-0.183478</td>\n",
       "      <td>0.4940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lin1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.027776</td>\n",
       "      <td>1.205657</td>\n",
       "      <td>-5.843505</td>\n",
       "      <td>6.552924e-08</td>\n",
       "      <td>-0.177880</td>\n",
       "      <td>0.4982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lin1</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.012747</td>\n",
       "      <td>1.179694</td>\n",
       "      <td>-5.259199</td>\n",
       "      <td>8.367818e-07</td>\n",
       "      <td>-0.166946</td>\n",
       "      <td>0.5021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lin1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.985659</td>\n",
       "      <td>1.142327</td>\n",
       "      <td>-4.473606</td>\n",
       "      <td>2.055800e-05</td>\n",
       "      <td>-0.156668</td>\n",
       "      <td>0.5094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lin1</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.968308</td>\n",
       "      <td>1.133278</td>\n",
       "      <td>-4.251347</td>\n",
       "      <td>4.817892e-05</td>\n",
       "      <td>-0.164970</td>\n",
       "      <td>0.4879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lin1</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.954484</td>\n",
       "      <td>1.126560</td>\n",
       "      <td>-4.068652</td>\n",
       "      <td>9.512833e-05</td>\n",
       "      <td>-0.172076</td>\n",
       "      <td>0.4929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lin1</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.946410</td>\n",
       "      <td>1.126560</td>\n",
       "      <td>-4.008388</td>\n",
       "      <td>1.185785e-04</td>\n",
       "      <td>-0.180150</td>\n",
       "      <td>0.4967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lin1</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.926750</td>\n",
       "      <td>1.126560</td>\n",
       "      <td>-3.849318</td>\n",
       "      <td>2.100248e-04</td>\n",
       "      <td>-0.199809</td>\n",
       "      <td>0.4937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lin2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.121617</td>\n",
       "      <td>1.191350</td>\n",
       "      <td>-0.729376</td>\n",
       "      <td>4.674937e-01</td>\n",
       "      <td>-0.069734</td>\n",
       "      <td>0.4931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lin2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.121622</td>\n",
       "      <td>1.191350</td>\n",
       "      <td>-0.729044</td>\n",
       "      <td>4.676961e-01</td>\n",
       "      <td>-0.069728</td>\n",
       "      <td>0.4968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lin2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.121140</td>\n",
       "      <td>1.187775</td>\n",
       "      <td>-0.686832</td>\n",
       "      <td>4.937943e-01</td>\n",
       "      <td>-0.066635</td>\n",
       "      <td>0.5094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lin2</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.137182</td>\n",
       "      <td>1.214252</td>\n",
       "      <td>-0.780698</td>\n",
       "      <td>4.368427e-01</td>\n",
       "      <td>-0.077070</td>\n",
       "      <td>0.4991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lin2</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.160277</td>\n",
       "      <td>1.250484</td>\n",
       "      <td>-0.887687</td>\n",
       "      <td>3.768606e-01</td>\n",
       "      <td>-0.090207</td>\n",
       "      <td>0.5030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lin2</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.147872</td>\n",
       "      <td>1.211103</td>\n",
       "      <td>-0.591464</td>\n",
       "      <td>5.555574e-01</td>\n",
       "      <td>-0.063231</td>\n",
       "      <td>0.4964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lin2</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1.141170</td>\n",
       "      <td>1.200767</td>\n",
       "      <td>-0.537859</td>\n",
       "      <td>5.918811e-01</td>\n",
       "      <td>-0.059597</td>\n",
       "      <td>0.5044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>lin2</td>\n",
       "      <td>20.00</td>\n",
       "      <td>1.095996</td>\n",
       "      <td>1.106738</td>\n",
       "      <td>-0.091930</td>\n",
       "      <td>9.269390e-01</td>\n",
       "      <td>-0.010743</td>\n",
       "      <td>0.5039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lin2</td>\n",
       "      <td>50.00</td>\n",
       "      <td>1.114064</td>\n",
       "      <td>1.103692</td>\n",
       "      <td>0.076612</td>\n",
       "      <td>9.390865e-01</td>\n",
       "      <td>0.010372</td>\n",
       "      <td>0.4947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lin3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.964314</td>\n",
       "      <td>2.089760</td>\n",
       "      <td>-1.991909</td>\n",
       "      <td>4.913535e-02</td>\n",
       "      <td>-0.125446</td>\n",
       "      <td>0.5026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lin3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.964378</td>\n",
       "      <td>2.089760</td>\n",
       "      <td>-1.991355</td>\n",
       "      <td>4.919718e-02</td>\n",
       "      <td>-0.125381</td>\n",
       "      <td>0.4997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lin3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.965059</td>\n",
       "      <td>2.089760</td>\n",
       "      <td>-1.985461</td>\n",
       "      <td>4.985931e-02</td>\n",
       "      <td>-0.124701</td>\n",
       "      <td>0.4935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>lin3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.965887</td>\n",
       "      <td>2.089760</td>\n",
       "      <td>-1.978120</td>\n",
       "      <td>5.069451e-02</td>\n",
       "      <td>-0.123872</td>\n",
       "      <td>0.5027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>lin3</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.960899</td>\n",
       "      <td>2.071188</td>\n",
       "      <td>-1.773394</td>\n",
       "      <td>7.923854e-02</td>\n",
       "      <td>-0.110289</td>\n",
       "      <td>0.5007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lin3</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.989063</td>\n",
       "      <td>2.112870</td>\n",
       "      <td>-2.050643</td>\n",
       "      <td>4.294400e-02</td>\n",
       "      <td>-0.123807</td>\n",
       "      <td>0.4860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>lin3</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1.971841</td>\n",
       "      <td>2.065351</td>\n",
       "      <td>-1.528275</td>\n",
       "      <td>1.296331e-01</td>\n",
       "      <td>-0.093510</td>\n",
       "      <td>0.4965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>lin3</td>\n",
       "      <td>20.00</td>\n",
       "      <td>1.955596</td>\n",
       "      <td>2.010656</td>\n",
       "      <td>-0.902985</td>\n",
       "      <td>3.687258e-01</td>\n",
       "      <td>-0.055060</td>\n",
       "      <td>0.5110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>lin3</td>\n",
       "      <td>50.00</td>\n",
       "      <td>1.930242</td>\n",
       "      <td>1.904078</td>\n",
       "      <td>0.380936</td>\n",
       "      <td>7.040673e-01</td>\n",
       "      <td>0.026164</td>\n",
       "      <td>0.4951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>lin4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.622893</td>\n",
       "      <td>1.648342</td>\n",
       "      <td>-0.429144</td>\n",
       "      <td>6.687510e-01</td>\n",
       "      <td>-0.025449</td>\n",
       "      <td>0.5052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>lin4</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.623311</td>\n",
       "      <td>1.648342</td>\n",
       "      <td>-0.422077</td>\n",
       "      <td>6.738838e-01</td>\n",
       "      <td>-0.025031</td>\n",
       "      <td>0.5075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lin4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.617307</td>\n",
       "      <td>1.627502</td>\n",
       "      <td>-0.168985</td>\n",
       "      <td>8.661533e-01</td>\n",
       "      <td>-0.010195</td>\n",
       "      <td>0.5013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>lin4</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.620885</td>\n",
       "      <td>1.624411</td>\n",
       "      <td>-0.058268</td>\n",
       "      <td>9.536523e-01</td>\n",
       "      <td>-0.003526</td>\n",
       "      <td>0.5009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>lin4</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.609315</td>\n",
       "      <td>1.569283</td>\n",
       "      <td>0.644743</td>\n",
       "      <td>5.205848e-01</td>\n",
       "      <td>0.040032</td>\n",
       "      <td>0.5032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>lin4</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.598280</td>\n",
       "      <td>1.497920</td>\n",
       "      <td>1.627890</td>\n",
       "      <td>1.067268e-01</td>\n",
       "      <td>0.100360</td>\n",
       "      <td>0.5037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>lin4</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1.589385</td>\n",
       "      <td>1.437927</td>\n",
       "      <td>2.423177</td>\n",
       "      <td>1.720195e-02</td>\n",
       "      <td>0.151458</td>\n",
       "      <td>0.4976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>lin4</td>\n",
       "      <td>20.00</td>\n",
       "      <td>1.596775</td>\n",
       "      <td>1.419583</td>\n",
       "      <td>2.704172</td>\n",
       "      <td>8.061543e-03</td>\n",
       "      <td>0.177192</td>\n",
       "      <td>0.4924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>lin4</td>\n",
       "      <td>50.00</td>\n",
       "      <td>1.599456</td>\n",
       "      <td>1.312717</td>\n",
       "      <td>3.897238</td>\n",
       "      <td>1.770717e-04</td>\n",
       "      <td>0.286739</td>\n",
       "      <td>0.5014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>lin5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.078528</td>\n",
       "      <td>0.545900</td>\n",
       "      <td>2.063597</td>\n",
       "      <td>4.167239e-02</td>\n",
       "      <td>0.532629</td>\n",
       "      <td>0.5019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>lin5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.065953</td>\n",
       "      <td>0.519749</td>\n",
       "      <td>2.117787</td>\n",
       "      <td>3.669782e-02</td>\n",
       "      <td>0.546204</td>\n",
       "      <td>0.4908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>lin5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.057185</td>\n",
       "      <td>0.491659</td>\n",
       "      <td>2.194004</td>\n",
       "      <td>3.057442e-02</td>\n",
       "      <td>0.565527</td>\n",
       "      <td>0.4944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>lin5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.039277</td>\n",
       "      <td>0.439596</td>\n",
       "      <td>2.294125</td>\n",
       "      <td>2.389605e-02</td>\n",
       "      <td>0.599682</td>\n",
       "      <td>0.4963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>lin5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.023400</td>\n",
       "      <td>0.353991</td>\n",
       "      <td>2.520309</td>\n",
       "      <td>1.332344e-02</td>\n",
       "      <td>0.669409</td>\n",
       "      <td>0.5033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>lin5</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.039317</td>\n",
       "      <td>0.282496</td>\n",
       "      <td>2.753163</td>\n",
       "      <td>7.022633e-03</td>\n",
       "      <td>0.756821</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>lin5</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1.134539</td>\n",
       "      <td>0.397079</td>\n",
       "      <td>2.598057</td>\n",
       "      <td>1.080546e-02</td>\n",
       "      <td>0.737461</td>\n",
       "      <td>0.4908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>lin5</td>\n",
       "      <td>20.00</td>\n",
       "      <td>1.126252</td>\n",
       "      <td>0.321847</td>\n",
       "      <td>2.782715</td>\n",
       "      <td>6.456507e-03</td>\n",
       "      <td>0.804404</td>\n",
       "      <td>0.4960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>lin5</td>\n",
       "      <td>50.00</td>\n",
       "      <td>1.218180</td>\n",
       "      <td>0.417181</td>\n",
       "      <td>2.399096</td>\n",
       "      <td>1.830718e-02</td>\n",
       "      <td>0.800999</td>\n",
       "      <td>0.4989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Reward Type  Alpha  Avg DR Rewards  Avg LinUCB Rewards  Paired t-stat  \\\n",
       "0         lin1   0.01        1.036236            1.219565      -6.047755   \n",
       "1         lin1   0.10        1.036087            1.219565      -6.047007   \n",
       "2         lin1   1.00        1.027776            1.205657      -5.843505   \n",
       "3         lin1   2.00        1.012747            1.179694      -5.259199   \n",
       "4         lin1   5.00        0.985659            1.142327      -4.473606   \n",
       "5         lin1  10.00        0.968308            1.133278      -4.251347   \n",
       "6         lin1  15.00        0.954484            1.126560      -4.068652   \n",
       "7         lin1  20.00        0.946410            1.126560      -4.008388   \n",
       "8         lin1  50.00        0.926750            1.126560      -3.849318   \n",
       "9         lin2   0.01        1.121617            1.191350      -0.729376   \n",
       "10        lin2   0.10        1.121622            1.191350      -0.729044   \n",
       "11        lin2   1.00        1.121140            1.187775      -0.686832   \n",
       "12        lin2   2.00        1.137182            1.214252      -0.780698   \n",
       "13        lin2   5.00        1.160277            1.250484      -0.887687   \n",
       "14        lin2  10.00        1.147872            1.211103      -0.591464   \n",
       "15        lin2  15.00        1.141170            1.200767      -0.537859   \n",
       "16        lin2  20.00        1.095996            1.106738      -0.091930   \n",
       "17        lin2  50.00        1.114064            1.103692       0.076612   \n",
       "18        lin3   0.01        1.964314            2.089760      -1.991909   \n",
       "19        lin3   0.10        1.964378            2.089760      -1.991355   \n",
       "20        lin3   1.00        1.965059            2.089760      -1.985461   \n",
       "21        lin3   2.00        1.965887            2.089760      -1.978120   \n",
       "22        lin3   5.00        1.960899            2.071188      -1.773394   \n",
       "23        lin3  10.00        1.989063            2.112870      -2.050643   \n",
       "24        lin3  15.00        1.971841            2.065351      -1.528275   \n",
       "25        lin3  20.00        1.955596            2.010656      -0.902985   \n",
       "26        lin3  50.00        1.930242            1.904078       0.380936   \n",
       "27        lin4   0.01        1.622893            1.648342      -0.429144   \n",
       "28        lin4   0.10        1.623311            1.648342      -0.422077   \n",
       "29        lin4   1.00        1.617307            1.627502      -0.168985   \n",
       "30        lin4   2.00        1.620885            1.624411      -0.058268   \n",
       "31        lin4   5.00        1.609315            1.569283       0.644743   \n",
       "32        lin4  10.00        1.598280            1.497920       1.627890   \n",
       "33        lin4  15.00        1.589385            1.437927       2.423177   \n",
       "34        lin4  20.00        1.596775            1.419583       2.704172   \n",
       "35        lin4  50.00        1.599456            1.312717       3.897238   \n",
       "36        lin5   0.01        1.078528            0.545900       2.063597   \n",
       "37        lin5   0.10        1.065953            0.519749       2.117787   \n",
       "38        lin5   1.00        1.057185            0.491659       2.194004   \n",
       "39        lin5   2.00        1.039277            0.439596       2.294125   \n",
       "40        lin5   5.00        1.023400            0.353991       2.520309   \n",
       "41        lin5  10.00        1.039317            0.282496       2.753163   \n",
       "42        lin5  15.00        1.134539            0.397079       2.598057   \n",
       "43        lin5  20.00        1.126252            0.321847       2.782715   \n",
       "44        lin5  50.00        1.218180            0.417181       2.399096   \n",
       "\n",
       "    Paired p-value  Observed Mean Diff  Bootstrap p-value  \n",
       "0     2.615536e-08           -0.183329             0.4973  \n",
       "1     2.624411e-08           -0.183478             0.4940  \n",
       "2     6.552924e-08           -0.177880             0.4982  \n",
       "3     8.367818e-07           -0.166946             0.5021  \n",
       "4     2.055800e-05           -0.156668             0.5094  \n",
       "5     4.817892e-05           -0.164970             0.4879  \n",
       "6     9.512833e-05           -0.172076             0.4929  \n",
       "7     1.185785e-04           -0.180150             0.4967  \n",
       "8     2.100248e-04           -0.199809             0.4937  \n",
       "9     4.674937e-01           -0.069734             0.4931  \n",
       "10    4.676961e-01           -0.069728             0.4968  \n",
       "11    4.937943e-01           -0.066635             0.5094  \n",
       "12    4.368427e-01           -0.077070             0.4991  \n",
       "13    3.768606e-01           -0.090207             0.5030  \n",
       "14    5.555574e-01           -0.063231             0.4964  \n",
       "15    5.918811e-01           -0.059597             0.5044  \n",
       "16    9.269390e-01           -0.010743             0.5039  \n",
       "17    9.390865e-01            0.010372             0.4947  \n",
       "18    4.913535e-02           -0.125446             0.5026  \n",
       "19    4.919718e-02           -0.125381             0.4997  \n",
       "20    4.985931e-02           -0.124701             0.4935  \n",
       "21    5.069451e-02           -0.123872             0.5027  \n",
       "22    7.923854e-02           -0.110289             0.5007  \n",
       "23    4.294400e-02           -0.123807             0.4860  \n",
       "24    1.296331e-01           -0.093510             0.4965  \n",
       "25    3.687258e-01           -0.055060             0.5110  \n",
       "26    7.040673e-01            0.026164             0.4951  \n",
       "27    6.687510e-01           -0.025449             0.5052  \n",
       "28    6.738838e-01           -0.025031             0.5075  \n",
       "29    8.661533e-01           -0.010195             0.5013  \n",
       "30    9.536523e-01           -0.003526             0.5009  \n",
       "31    5.205848e-01            0.040032             0.5032  \n",
       "32    1.067268e-01            0.100360             0.5037  \n",
       "33    1.720195e-02            0.151458             0.4976  \n",
       "34    8.061543e-03            0.177192             0.4924  \n",
       "35    1.770717e-04            0.286739             0.5014  \n",
       "36    4.167239e-02            0.532629             0.5019  \n",
       "37    3.669782e-02            0.546204             0.4908  \n",
       "38    3.057442e-02            0.565527             0.4944  \n",
       "39    2.389605e-02            0.599682             0.4963  \n",
       "40    1.332344e-02            0.669409             0.5033  \n",
       "41    7.022633e-03            0.756821             0.5000  \n",
       "42    1.080546e-02            0.737461             0.4908  \n",
       "43    6.456507e-03            0.804404             0.4960  \n",
       "44    1.830718e-02            0.800999             0.4989  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the backtest\n",
    "alpha_values = [0.01, 0.1, 1.0, 10.0, 50.0]\n",
    "imbalance_ratio = 1.5  # Test different imbalance levels as desired\n",
    "results_all = pd.DataFrame()\n",
    "\n",
    "for reward_type in ['lin1', 'lin2', 'lin3', 'lin4', 'lin5']:\n",
    "    result_df = run_backtest_with_alpha_test(\n",
    "        X, total_samples=1000, n_actions=2, optimal_actions=optimal_actions, \n",
    "        context_dim=X.shape[1], reward_type=reward_type, \n",
    "        alpha_values=alpha_values, imbalance_ratio=imbalance_ratio)\n",
    "    results_all = pd.concat([results_all, result_df], ignore_index=True)\n",
    "\n",
    "# Display or save final results DataFrame\n",
    "results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163c502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reward Type</th>\n",
       "      <th>Alpha Pair</th>\n",
       "      <th>Avg DR Rewards (No Exploration)</th>\n",
       "      <th>Avg DR Rewards (With Exploration)</th>\n",
       "      <th>Paired t-stat</th>\n",
       "      <th>Paired p-value</th>\n",
       "      <th>Observed Mean Diff</th>\n",
       "      <th>Bootstrap p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lin1</td>\n",
       "      <td>0.01 vs 5.0</td>\n",
       "      <td>1.002683</td>\n",
       "      <td>0.987286</td>\n",
       "      <td>0.971970</td>\n",
       "      <td>0.333434</td>\n",
       "      <td>-0.015397</td>\n",
       "      <td>0.4920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lin1</td>\n",
       "      <td>0.01 vs 20.0</td>\n",
       "      <td>1.002683</td>\n",
       "      <td>0.946410</td>\n",
       "      <td>2.827290</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>-0.056273</td>\n",
       "      <td>0.4927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lin1</td>\n",
       "      <td>0.01 vs 50.0</td>\n",
       "      <td>1.002683</td>\n",
       "      <td>0.921676</td>\n",
       "      <td>3.445175</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>-0.081006</td>\n",
       "      <td>0.4955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lin2</td>\n",
       "      <td>0.01 vs 5.0</td>\n",
       "      <td>1.165367</td>\n",
       "      <td>1.129588</td>\n",
       "      <td>1.498521</td>\n",
       "      <td>0.137180</td>\n",
       "      <td>-0.035779</td>\n",
       "      <td>0.5203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lin2</td>\n",
       "      <td>0.01 vs 20.0</td>\n",
       "      <td>1.165367</td>\n",
       "      <td>1.113143</td>\n",
       "      <td>1.575056</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>-0.052224</td>\n",
       "      <td>0.5051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lin2</td>\n",
       "      <td>0.01 vs 50.0</td>\n",
       "      <td>1.165367</td>\n",
       "      <td>1.119896</td>\n",
       "      <td>1.049447</td>\n",
       "      <td>0.296528</td>\n",
       "      <td>-0.045471</td>\n",
       "      <td>0.5076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lin3</td>\n",
       "      <td>0.01 vs 5.0</td>\n",
       "      <td>2.015294</td>\n",
       "      <td>1.990365</td>\n",
       "      <td>1.161197</td>\n",
       "      <td>0.248354</td>\n",
       "      <td>-0.024929</td>\n",
       "      <td>0.5223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lin3</td>\n",
       "      <td>0.01 vs 20.0</td>\n",
       "      <td>2.015294</td>\n",
       "      <td>1.972605</td>\n",
       "      <td>1.747295</td>\n",
       "      <td>0.083687</td>\n",
       "      <td>-0.042690</td>\n",
       "      <td>0.4974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lin3</td>\n",
       "      <td>0.01 vs 50.0</td>\n",
       "      <td>2.015294</td>\n",
       "      <td>2.002913</td>\n",
       "      <td>0.555503</td>\n",
       "      <td>0.579803</td>\n",
       "      <td>-0.012381</td>\n",
       "      <td>0.5094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lin4</td>\n",
       "      <td>0.01 vs 5.0</td>\n",
       "      <td>1.530217</td>\n",
       "      <td>1.523874</td>\n",
       "      <td>0.441207</td>\n",
       "      <td>0.660025</td>\n",
       "      <td>-0.006343</td>\n",
       "      <td>0.5327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lin4</td>\n",
       "      <td>0.01 vs 20.0</td>\n",
       "      <td>1.530217</td>\n",
       "      <td>1.460641</td>\n",
       "      <td>2.515793</td>\n",
       "      <td>0.013485</td>\n",
       "      <td>-0.069576</td>\n",
       "      <td>0.5113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lin4</td>\n",
       "      <td>0.01 vs 50.0</td>\n",
       "      <td>1.530217</td>\n",
       "      <td>1.498109</td>\n",
       "      <td>1.170961</td>\n",
       "      <td>0.244425</td>\n",
       "      <td>-0.032108</td>\n",
       "      <td>0.4983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lin5</td>\n",
       "      <td>0.01 vs 5.0</td>\n",
       "      <td>0.936085</td>\n",
       "      <td>0.843307</td>\n",
       "      <td>1.060338</td>\n",
       "      <td>0.291571</td>\n",
       "      <td>-0.092777</td>\n",
       "      <td>0.5353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lin5</td>\n",
       "      <td>0.01 vs 20.0</td>\n",
       "      <td>0.936085</td>\n",
       "      <td>0.755919</td>\n",
       "      <td>1.667394</td>\n",
       "      <td>0.098597</td>\n",
       "      <td>-0.180166</td>\n",
       "      <td>0.5189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lin5</td>\n",
       "      <td>0.01 vs 50.0</td>\n",
       "      <td>0.936085</td>\n",
       "      <td>0.774443</td>\n",
       "      <td>1.296452</td>\n",
       "      <td>0.197834</td>\n",
       "      <td>-0.161641</td>\n",
       "      <td>0.5131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Reward Type    Alpha Pair  Avg DR Rewards (No Exploration)  \\\n",
       "0         lin1   0.01 vs 5.0                         1.002683   \n",
       "1         lin1  0.01 vs 20.0                         1.002683   \n",
       "2         lin1  0.01 vs 50.0                         1.002683   \n",
       "3         lin2   0.01 vs 5.0                         1.165367   \n",
       "4         lin2  0.01 vs 20.0                         1.165367   \n",
       "5         lin2  0.01 vs 50.0                         1.165367   \n",
       "6         lin3   0.01 vs 5.0                         2.015294   \n",
       "7         lin3  0.01 vs 20.0                         2.015294   \n",
       "8         lin3  0.01 vs 50.0                         2.015294   \n",
       "9         lin4   0.01 vs 5.0                         1.530217   \n",
       "10        lin4  0.01 vs 20.0                         1.530217   \n",
       "11        lin4  0.01 vs 50.0                         1.530217   \n",
       "12        lin5   0.01 vs 5.0                         0.936085   \n",
       "13        lin5  0.01 vs 20.0                         0.936085   \n",
       "14        lin5  0.01 vs 50.0                         0.936085   \n",
       "\n",
       "    Avg DR Rewards (With Exploration)  Paired t-stat  Paired p-value  \\\n",
       "0                            0.987286       0.971970        0.333434   \n",
       "1                            0.946410       2.827290        0.005681   \n",
       "2                            0.921676       3.445175        0.000839   \n",
       "3                            1.129588       1.498521        0.137180   \n",
       "4                            1.113143       1.575056        0.118434   \n",
       "5                            1.119896       1.049447        0.296528   \n",
       "6                            1.990365       1.161197        0.248354   \n",
       "7                            1.972605       1.747295        0.083687   \n",
       "8                            2.002913       0.555503        0.579803   \n",
       "9                            1.523874       0.441207        0.660025   \n",
       "10                           1.460641       2.515793        0.013485   \n",
       "11                           1.498109       1.170961        0.244425   \n",
       "12                           0.843307       1.060338        0.291571   \n",
       "13                           0.755919       1.667394        0.098597   \n",
       "14                           0.774443       1.296452        0.197834   \n",
       "\n",
       "    Observed Mean Diff  Bootstrap p-value  \n",
       "0            -0.015397             0.4920  \n",
       "1            -0.056273             0.4927  \n",
       "2            -0.081006             0.4955  \n",
       "3            -0.035779             0.5203  \n",
       "4            -0.052224             0.5051  \n",
       "5            -0.045471             0.5076  \n",
       "6            -0.024929             0.5223  \n",
       "7            -0.042690             0.4974  \n",
       "8            -0.012381             0.5094  \n",
       "9            -0.006343             0.5327  \n",
       "10           -0.069576             0.5113  \n",
       "11           -0.032108             0.4983  \n",
       "12           -0.092777             0.5353  \n",
       "13           -0.180166             0.5189  \n",
       "14           -0.161641             0.5131  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "def run_backtest_with_alpha_test_with_exploration(X, total_samples, n_actions, optimal_actions, context_dim, reward_type='linear', alpha_values=[0.1, 0.5, 1.0], epochs=20):\n",
    "    X_train, X_val, historical_actions, historical_rewards, optimal_actions_val, rewards_val = generate_data_and_split(\n",
    "        X, total_samples, n_actions, optimal_actions, reward_type)\n",
    "\n",
    "    # Convert PyTorch tensors to NumPy arrays if needed\n",
    "    if isinstance(X_train, torch.Tensor):\n",
    "        X_train = X_train.detach().numpy()\n",
    "    if isinstance(X_val, torch.Tensor):\n",
    "        X_val = X_val.detach().numpy()\n",
    "    if isinstance(historical_rewards, torch.Tensor):\n",
    "        historical_rewards = historical_rewards.detach().numpy()\n",
    "    if isinstance(historical_actions, torch.Tensor):\n",
    "        historical_actions = historical_actions.detach().numpy()\n",
    "\n",
    "    # Generate true rewards based on optimal actions\n",
    "    all_true_rewards = np.zeros((X_val.shape[0], n_actions))\n",
    "    reward_gen = RewardGenerator()\n",
    "    \n",
    "    for action in range(n_actions):\n",
    "        for i, opt_action in enumerate(optimal_actions_val):\n",
    "            if reward_type == 'lin1':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards1(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin2':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards2(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin3':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards3(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin4':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards4(X_val[i], action, opt_action)\n",
    "            elif reward_type == 'lin5':\n",
    "                all_true_rewards[i, action] = reward_gen.generate_rewards5(X_val[i], action, opt_action)\n",
    "\n",
    "    # Create an empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['Reward Type', 'Alpha Pair', 'Avg DR Rewards (No Exploration)', 'Avg DR Rewards (With Exploration)', 'Paired t-stat', 'Paired p-value', 'Observed Mean Diff', 'Bootstrap p-value'])\n",
    "\n",
    "    # Store DR rewards for each alpha to compare later\n",
    "    dr_rewards_dict = {}\n",
    "\n",
    "    for alpha in alpha_values:\n",
    "        # Initialize LinUCB model with current alpha\n",
    "        lin_UCB_DR_model = LinUCB_DR(n_actions, context_dim, alpha=alpha)\n",
    "        lin_UCB_DR_model.calculate_propensity_scores(historical_actions)  # Calculate propensities internally\n",
    "        \n",
    "        # Train LinUCB model on historical data\n",
    "        for i in range(len(historical_actions)):\n",
    "            lin_UCB_DR_model.update(historical_actions[i], historical_rewards[i, historical_actions[i]], X_train[i])\n",
    "        \n",
    "        # Predict with LinUCB on validation set\n",
    "        lin_UCB_preds = lin_UCB_DR_model.predict(X_val)\n",
    "        lin_UCB_preds = np.argmax(lin_UCB_preds, axis=1)\n",
    "        \n",
    "        # Calculate doubly robust rewards\n",
    "        dr_rewards = lin_UCB_DR_model.doubly_robust_estimator(X_val, lin_UCB_preds, historical_rewards, historical_actions, all_true_rewards)\n",
    "        dr_rewards_avg = np.mean(dr_rewards)\n",
    "        \n",
    "        # Store the DR rewards for the current alpha value\n",
    "        dr_rewards_dict[alpha] = dr_rewards\n",
    "\n",
    "    # Perform paired t-test and bootstrap test between alpha=0.01 (without exploration) and other alphas\n",
    "    alpha_no_exploration = 0.01\n",
    "    dr_rewards_no_exploration = dr_rewards_dict[alpha_no_exploration]\n",
    "\n",
    "    for alpha_with_exploration in alpha_values:\n",
    "        if alpha_with_exploration == alpha_no_exploration:\n",
    "            continue  # Skip comparison of 0.01 with itself\n",
    "\n",
    "        dr_rewards_with_exploration = dr_rewards_dict[alpha_with_exploration]\n",
    "\n",
    "        # Paired t-test between DR rewards with and without exploration\n",
    "        t_stat, p_value_ttest = stats.ttest_rel(dr_rewards_no_exploration, dr_rewards_with_exploration)\n",
    "\n",
    "        # Bootstrap Test\n",
    "        n_bootstrap = 10000\n",
    "        differences = dr_rewards_with_exploration - dr_rewards_no_exploration\n",
    "        observed_mean_diff = np.mean(differences)\n",
    "        \n",
    "        # Bootstrap sampling\n",
    "        bootstrap_means = np.array([\n",
    "            np.mean(np.random.choice(differences, size=len(differences), replace=True)) \n",
    "            for _ in range(n_bootstrap)\n",
    "        ])\n",
    "        \n",
    "        # Calculate the p-value as the proportion of bootstrap samples with a mean greater than or equal to the observed mean difference\n",
    "        p_value_bootstrap = np.mean(bootstrap_means >= observed_mean_diff)\n",
    "        \n",
    "        # Append comparison results to the DataFrame\n",
    "        temp_df = pd.DataFrame([{\n",
    "            'Reward Type': reward_type,\n",
    "            'Alpha Pair': f\"{alpha_no_exploration} vs {alpha_with_exploration}\",\n",
    "            'Avg DR Rewards (No Exploration)': np.mean(dr_rewards_no_exploration),\n",
    "            'Avg DR Rewards (With Exploration)': np.mean(dr_rewards_with_exploration),\n",
    "            'Paired t-stat': t_stat,\n",
    "            'Paired p-value': p_value_ttest,\n",
    "            'Observed Mean Diff': observed_mean_diff,\n",
    "            'Bootstrap p-value': p_value_bootstrap\n",
    "        }])\n",
    "        \n",
    "        results_df = pd.concat([results_df, temp_df], ignore_index=True)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Running the function with a loop over different reward types and alpha values\n",
    "alpha_values = [0.01, 5.0, 20.0, 50.0]\n",
    "results_all = pd.DataFrame()\n",
    "for reward_type in ['lin1', 'lin2', 'lin3', 'lin4', 'lin5']:\n",
    "    result_df = run_backtest_with_alpha_test_with_exploration(X, total_samples=1000, n_actions=2, optimal_actions=optimal_actions, context_dim=X.shape[1], reward_type=reward_type, alpha_values=alpha_values)\n",
    "    results_all = pd.concat([results_all, result_df], ignore_index=True)\n",
    "\n",
    "# Display the final results DataFrame\n",
    "display(results_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2653c329-c282-4e18-a525-9c5209d0efe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
